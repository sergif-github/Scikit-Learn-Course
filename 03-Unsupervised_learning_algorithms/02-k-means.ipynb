{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Part 2: K-means Clustering\n",
    "\n",
    "In this part, we will explore k-means clustering, a popular unsupervised learning algorithm used for clustering tasks. k-means clustering aims to partition a dataset into k distinct clusters based on the similarity of the data points.\n",
    "\n",
    "### 2.1 Understanding k-means Clustering\n",
    "\n",
    "K-means clustering is a widely used unsupervised machine learning algorithm that aims to partition a dataset into a predefined number of clusters or groups. The primary goal is to group similar data points together and identify underlying patterns within the data without relying on pre-existing labels. \n",
    "\n",
    "The key idea behind k-means clustering is to minimize the total distance between each data point and the centroid of its assigned cluster. Each cluster is represented by its centroid, which is the mean of all data points assigned to that cluster. The algorithm iteratively updates the cluster assignments and recalculates the centroids until convergence, where the centroids no longer change significantly. \n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- Clusters: K-means divides the data into \"k\" clusters, where \"k\" is a user-defined parameter. Each cluster represents a group of data points that are close to each other in feature space.\n",
    "- Centroids: The algorithm initializes \"k\" centroids, one for each cluster. Centroids are the center points of the clusters and are updated during the training process.\n",
    "- Assignment Step: In each iteration, each data point is assigned to the cluster whose centroid is closest to it. This assignment is based on a distance metric, often Euclidean distance.\n",
    "- Update Step: After all data points are assigned to clusters, the centroids are updated to the mean of the data points within each cluster.\n",
    "- Convergence: The assignment and update steps are repeated until convergence, typically defined by a convergence criterion, such as a small change in centroids or a fixed number of iterations.\n",
    "- Random Initialization: K-means is sensitive to the initial placement of centroids, so it's common to run the algorithm multiple times with different initializations and select the best result.\n",
    "\n",
    "It's important to note that k-means clustering has some limitations. It assumes that clusters are isotropic, have equal variance, and are equally sized. The algorithm may also be sensitive to outliers. If these assumptions are violated, other clustering algorithms or modifications of k-means, such as Gaussian Mixture Models, may be more appropriate.\n",
    "\n",
    "### 2.2 Training\n",
    "\n",
    "To apply the k-means algorithm, we need an unlabeled dataset. The model learns by iteratively updating the cluster assignments and centroids based on the distance between data points and centroids.\n",
    "\n",
    "One of the challenges in k-means clustering is determining the optimal number of clusters, k. Choosing the right value of k is crucial to obtain meaningful and interpretable clusters. There are various methods to estimate the optimal value of k, such as the elbow method, silhouette analysis, or domain knowledge.\n",
    "\n",
    "It is important to scale the features before applying k-means clustering to ensure that all features contribute equally to the clustering process. StandardScaler or MinMaxScaler can be used to scale the features appropriately.\n",
    "\n",
    "Additionally, k-means clustering is sensitive to the initialization of centroids. Scikit-Learn uses the k-means++ initialization method by default, which is more effective than random initialization in most cases.\n",
    "\n",
    "Once trained, we can use the k-means model to predict the cluster labels for new, unseen data points. The model assigns each data point to the nearest centroid, based on the distance metric used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, n_features=2, centers=3, cluster_std=2.0, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "db_index = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.2f}\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=20)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=40, marker='X', label='Cluster Centers')\n",
    "plt.title('K-means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates K-means clustering on synthetic data with three clusters. It calculates and prints two clustering evaluation metrics, the Silhouette Score and Davies-Bouldin Index, to assess the quality of the clustering. \n",
    "\n",
    "The Silhouette Score measures how well-separated and cohesive the clusters are. In this example, the Silhouette Score is approximately 0.58, indicating reasonably well-defined clusters.\n",
    "The Davies-Bouldin Index quantifies the average similarity ratio between clusters. In this case, it's approximately 0.60, suggesting good clustering quality.\n",
    "These metrics provide insights into the clustering quality, and the visualization shows the data points and cluster centers.\n",
    "\n",
    "### 2.3 Summary\n",
    "\n",
    "K-means clustering is a fundamental unsupervised learning technique for partitioning data into clusters based on similarity. It's widely used for various applications, including customer segmentation, image compression, and anomaly detection. Understanding the algorithm's inner workings, appropriate parameter selection, and evaluation are crucial for successful clustering. Keep in mind that k-means has limitations, such as its sensitivity to cluster shape and the need to specify the number of clusters in advance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
