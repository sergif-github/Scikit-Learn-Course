{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Part 15: Decision Trees\n",
    "\n",
    "In this section, we will explore Decision Trees, powerful supervised learning algorithms used for both classification and regression tasks. Decision Trees create a tree-like model of decisions for improved accuracy.\n",
    "\n",
    "### 15.1 Understanding Decision Trees\n",
    "\n",
    "A decision tree consists of nodes, including a root node, internal nodes, and leaf nodes. The root node represents the entire dataset, and internal nodes represent subsets of data based on feature values. Leaf nodes contain the final prediction or class label.\n",
    "\n",
    "Decision trees work by recursively partitioning the dataset into subsets based on the values of input features, ultimately leading to a tree-like structure where each leaf node represents a class label (in classification) or a predicted numerical value (in regression). The choice of which feature to split on and the value(s) at which to split is based on a splitting criterion. For classification, common criteria include Gini impurity and entropy, while for regression, mean squared error (MSE) is often used. The tree can grow until a specified depth, or until the minimum number of samples required to split a node is reached.\n",
    "\n",
    "To make a prediction, an input sample traverses the decision tree from the root to a leaf node. In classification, the majority class in a leaf node is assigned as the prediction, while in regression, the predicted value is the mean (or another measure) of the target values in the leaf node.\n",
    "\n",
    "Decision trees provide a measure of feature importance, which indicates how much each feature contributes to the model's predictions. This can be useful for feature selection and understanding the most influential factors in a model.\n",
    "\n",
    "### 15.2 Training and Evaluation\n",
    "\n",
    "To train a Decision Tree model, we need a labeled dataset with the target variable and the corresponding feature values. The model learns the decision rules based on the training data to make predictions.\n",
    "\n",
    "Decision Trees have several hyperparameters that control the model's behavior, such as the maximum depth of the tree, the number of trees in the forest, and the criterion used for splitting nodes. Tuning these hyperparameters can significantly impact the model's performance.\n",
    "\n",
    "Once trained, we can evaluate the model's performance using evaluation metrics suitable for classification or regression tasks, such as accuracy, precision, recall, F1-score, or mean squared error.\n",
    "\n",
    "#### DecisionTreeClassifier Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plot_tree(clf, filled=True, feature_names=cancer.feature_names.tolist(), class_names=cancer.target_names.tolist())\n",
    "plt.title('Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example showcases how to build, train, and evaluate a decision tree classifier using scikit-learn.\n",
    "\n",
    "First we load the Iris dataset and split it into training and testing sets and we create a DecisionTreeClassifier and fit it to the training data. We make predictions on the test data and calculate accuracy. Finally a visual representation of the decision tree is displayed.\n",
    "\n",
    "#### DecisionTreeRegressor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Order y_pred based on X_test\n",
    "sorted_indices = X_test.ravel().argsort()\n",
    "X_test_sorted = X_test[sorted_indices]\n",
    "y_pred_sorted = y_pred[sorted_indices]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (RÂ²): {r2:.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test_sorted, y_pred_sorted, color=\"cornflowerblue\", linewidth=2, label=\"prediction\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plot_tree(clf, filled=True, feature_names=cancer.feature_names.tolist(), class_names=cancer.target_names.tolist())\n",
    "plt.title('Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we generate synthetic data with some noise and use a DecisionTreeRegressor to learn the underlying pattern. The max_depth parameter controls the depth of the tree, which can be adjusted depending on your specific regression problem. The visualization shows the original data points and the regression line predicted by the decision tree.\n",
    "\n",
    "### 15.3 Summary\n",
    "\n",
    "A Decision Tree is a versatile machine learning algorithm used for both classification and regression tasks. It partitions data into subsets based on features, aiming to maximize information gain or reduce impurity at each node. Decision Trees are interpretable, making them valuable for understanding decision processes. However, they can be prone to overfitting complex data. Techniques like pruning and limiting tree depth help mitigate this. Decision Trees are a foundational component in ensemble methods like Random Forests and Gradient Boosting, enhancing predictive power. They are widely used in various domains due to their simplicity, effectiveness, and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
