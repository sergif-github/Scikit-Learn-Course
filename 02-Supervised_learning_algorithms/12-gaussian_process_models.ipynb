{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 12: Gaussian Process models\n",
    "\n",
    "In this part, we will explore Gaussian Process (GP) models, a flexible and powerful class of probabilistic models that can be used for both regression and classification tasks. Gaussian Process models provide a non-parametric approach to modeling data, allowing for uncertainty estimation and capturing complex relationships.\n",
    "\n",
    "### 12.1 Understanding Gaussian Process (GP) models\n",
    "\n",
    "Gaussian Process (GP) models are a family of probabilistic models that define a distribution over functions. Instead of modeling the data points directly, GP models capture the distribution over possible functions that could explain the data.\n",
    "\n",
    "A Gaussian Process is fully specified by its mean function and covariance function (also called kernel function). The mean function represents the expected value of the function, while the covariance function characterizes the similarity between input points. Different covariance functions capture different types of relationships, such as smoothness, periodicity, or non-linear interactions.\n",
    "\n",
    "While they are computationally intensive for large datasets, their probabilistic nature and ability to handle uncertainty make them invaluable in numerous machine learning applications.\n",
    "\n",
    "The GP doesn't \"fit\" a specific function in the traditional sense, but it provides a probabilistic framework for estimating function values at new points and quantifying the uncertainty associated with those predictions. The predicted values are expected to follow the underlying pattern found in the data, but they also account for the inherent uncertainty in the modeling process.\n",
    "\n",
    "### 12.2 Training and Evaluation\n",
    "\n",
    "To train a Gaussian Process model, we need a labeled dataset with the target variable and the corresponding feature values. The model learns by estimating the mean and covariance functions based on the training data.\n",
    "\n",
    "Once trained, we can use the Gaussian Process model to make predictions for new, unseen data points. The model provides not only the predicted values but also the uncertainty associated with each prediction. This uncertainty estimation is a key advantage of Gaussian Process models.\n",
    "\n",
    "The choice of the kernel for a Gaussian Process (GP) model depends on the specific characteristics of your data and the problem you are trying to solve. Here are some general guidelines to help you choose an appropriate kernel:\n",
    "\n",
    "- RBF (Radial Basis Function) Kernel:<br>\n",
    "The RBF kernel, also known as the Gaussian kernel,  is smooth and continuous, making it suitable for modeling functions that exhibit gradual changes and smooth transitions. Is stationary, which means it assumes that the underlying function's statistical properties do not change with changes in the input space. Is known as a \"universal approximator\" because it can approximate any continuous function with enough data and appropriate hyperparameter tuning.\n",
    "<br>The kernel__length_scale hyperparameter represents the characteristic length scale of the RBF kernel. It controls the smoothness of the underlying function that the GP is trying to model.\n",
    "<br>A small length scale corresponds to a very \"wiggly\" or oscillatory function because it considers nearby data points as similar.\n",
    "<br>A large length scale corresponds to a smoother function because it considers a broader range of data points as similar.\n",
    "<br>The kernel__length_scale_bounds hyperparameter specifies the bounds or constraints on the possible values of the length scale. It is often used to guide the optimization process during hyperparameter tuning to prevent the length scale from taking extreme values that might lead to overfitting or underfitting.\n",
    "For example, if you set kernel__length_scale_bounds to (1e-5, 1e5), you're constraining the length scale to be within this range during optimization. This prevents the optimizer from exploring length scales that are too extreme.\n",
    "\n",
    "- Matern Kernel:<br>\n",
    "Choose the Matern kernel with an appropriate value of the smoothness parameter (nu) based on your knowledge of the problem. Use when you have some prior knowledge about the differentiability of the underlying function.<br>\n",
    "It is particularly suitable for situations where you have some prior knowledge about the differentiability or smoothness of the underlying function. It has a hyperparameter, often denoted as \"ν\" (nu), that controls the smoothness.<br>\n",
    "ν = 0.5: Rough and not differentiable (similar to the absolute exponential kernel).<br>\n",
    "ν = 1.5: Once differentiable (similar to the RBF kernel).<br>\n",
    "ν = ∞: Infinitely differentiable (similar to the linear kernel).\n",
    "\n",
    "- Rational Quadratic Kernel:<br>\n",
    "It's a versatile kernel that can be useful in scenarios where you suspect the underlying function may have varying degrees of smoothness and is not well-modeled by a single length scale. <br>The primary hyperparameters associated with the Rational Quadratic Kernel are:<br>\n",
    "Alpha (α): This hyperparameter, often denoted as α, determines the scale of the kernel and controls the relative contribution of large and small length scales. It influences the overall shape of the kernel function.<br>\n",
    "Length Scale (l): The length scale hyperparameter determines how quickly the kernel's correlation between data points decreases with increasing distance. A smaller length scale results in a more rapidly decaying correlation, while a larger length scale implies a more slowly decaying correlation.<br>\n",
    "\n",
    "- Exp-Sine-Squared Kernel:<br>The Exponentiated Sine-Squared Kernel, often denoted as the Exp-Sine-Squared Kernel, is a specific type of kernel used in Gaussian Process models. It's particularly useful when dealing with data that exhibits periodic or oscillatory patterns. <br>The Exp-Sine-Squared Kernel has two primary hyperparameters:<br>Length Scale (l): The length scale determines how rapidly the kernel's correlation between data points decreases with increasing distance. It effectively controls the smoothness of the function. Smaller values of l result in more rapid oscillations, while larger values smooth out the function.<br>Periodicity (p): The periodicity parameter controls the frequency of the oscillations. Larger values of p correspond to lower frequencies, while smaller values result in higher frequencies.\n",
    "\n",
    "- Dot Product:<br>\n",
    "The Dot Product Kernel is a simple yet versatile kernel used in Gaussian Process models. It's typically used when you want to allow your Gaussian Process to fit a wide range of functions without making strong assumptions about the underlying data structure.<br>The Dot Product Kernel doesn't have many hyperparameters to tune. One key hyperparameter is the \"constant_value\" or \"sigma_0,\" which controls the overall scale of the kernel's influence on the predictions.\n",
    "\n",
    "#### Example using GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data with a more periodic sinusoidal pattern\n",
    "np.random.seed(0)\n",
    "X = np.sort(10 * np.random.rand(5000, 1), axis=0)\n",
    "y = (np.sin(4*X)).ravel()\n",
    "y += 0.3 * np.random.randn(5000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Gaussian Process Regressor with the RBF kernel\n",
    "gp_regressor = GaussianProcessRegressor(kernel=RBF())\n",
    "param_grid = {\n",
    "    'kernel__length_scale': [1e-20, 1e-15, 1e-10, 1e-05],  # Add more values\n",
    "    'kernel__length_scale_bounds': [(1e-1, 1e1)],  # Add more bounds\n",
    "}\n",
    "grid_search = GridSearchCV(gp_regressor, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "best_gp_regressor = grid_search.best_estimator_\n",
    "y_pred_mean, y_pred_std = best_gp_regressor.predict(X_test, return_std=True)\n",
    "mae = mean_absolute_error(y_test, y_pred_mean)\n",
    "mse = mean_squared_error(y_test, y_pred_mean)\n",
    "r2 = r2_score(y_test, y_pred_mean)\n",
    "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "print(f'R-squared (R²): {r2:.2f}')\n",
    "\n",
    "# Sort the predictions based on the corresponding X_test values\n",
    "sorted_indices = X_test.ravel().argsort()\n",
    "X_test_sorted = X_test[sorted_indices]\n",
    "y_pred_mean_sorted = y_pred_mean[sorted_indices]\n",
    "y_pred_std_sorted = y_pred_std[sorted_indices]\n",
    "\n",
    "# Plot the results in two subplots\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, c='r', label='Training Data', s=5)\n",
    "plt.scatter(X_test, y_test, c='g', label='Testing Data', s=5)\n",
    "plt.title('Training and Test Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, c='g', label='Testing Data', s=5)\n",
    "plt.plot(X_test_sorted, y_pred_mean_sorted, 'k', lw=1, label=f'Kernel: RBF')\n",
    "plt.fill_between(X_test_sorted.ravel(), y_pred_mean_sorted - y_pred_std_sorted, y_pred_mean_sorted + y_pred_std_sorted, color='gray', alpha=0.5)\n",
    "plt.title('Predicted Mean and Std Deviation')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we explored the use of Gaussian Process Regression with the Radial Basis Function (RBF) kernel to model a synthetic dataset exhibiting a periodic sinusoidal pattern with added noise. The goal was to demonstrate how to tune hyperparameters for the RBF kernel using GridSearchCV and evaluate the model's performance.\n",
    "\n",
    "After a careful hyperparameter search, the RBF kernel successfully captured the underlying data distribution. Metrics indicate that the model provides a reasonably accurate representation of the data. The low MAE and MSE values suggest that the model's predictions are close to the actual target values. Additionally, the high R² score of 0.84 indicates that a significant portion of the variance in the data is explained by the model.\n",
    "\n",
    "#### Example using GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0)\n",
    "gp_classifier = GaussianProcessClassifier(kernel=kernel)\n",
    "gp_classifier.fit(X_train, y_train)\n",
    "y_pred = gp_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = gp_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Gaussian Process Classifier Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a synthetic classification dataset using make_classification. The dataset is split into training and testing sets. We define the GaussianProcessClassifier with an RBF kernel and fit it to the training data.\n",
    "We predict labels for the testing data and calculate the accuracy and classification report. Finally, we plot the decision boundary of the classifier along with the data points to visualize how the model separates the classes.\n",
    "\n",
    "### 12.3 Summary\n",
    "\n",
    "Gaussian Process (GP) models are a powerful and flexible class of statistical models used in machine learning for regression and classification tasks. Unlike many other algorithms, GPs provide a probabilistic framework for modeling relationships in data. They assume that the data points are generated from a multivariate Gaussian distribution and capture the uncertainty associated with predictions.\n",
    "\n",
    "In regression, GPs estimate a smooth function that best fits the data while providing confidence intervals for predictions. For classification, Gaussian Process Classifiers (GPCs) extend GPs to handle discrete class labels, modeling class probabilities.\n",
    "\n",
    "Key features of GPs include their ability to adapt to different data distributions, handle non-linear relationships, and quantify uncertainty in predictions. However, they may be computationally expensive for large datasets. GPs are powerful tools for solving problems where modeling complex, non-parametric relationships with uncertainty estimates is crucial, such as in geostatistics, time series analysis, and Bayesian optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
