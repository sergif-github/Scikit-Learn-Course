{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Part 18: Multiclass and multioutput algorithms\n",
    "\n",
    "Scikit-learn offers a variety of algorithms and techniques for tackling multi-class and multi-output problems in machine learning. Here, we'll delve into the theory and concepts behind these algorithms.\n",
    "\n",
    "### 18.1 Multi-Class Classification\n",
    "\n",
    "Multi-class classification refers to the task of classifying data points into more than two distinct classes or categories. Several approaches are used for multi-class classification in scikit-learn:\n",
    "\n",
    "- One-vs-Rest (OvR) or One-vs-All (OvA)<br> In OvR, a binary classifier is trained for each class against all the other classes. During prediction, each classifier produces a decision score, and the class with the highest score is chosen as the prediction. This method is suitable when classes are not mutually exclusive.\n",
    "\n",
    "- Multinomial (Softmax) Logistic Regression<br> This is a generalized logistic regression for multi-class classification. It models the probabilities of each class directly and uses a softmax function to convert scores into class probabilities. This approach is well-suited for problems where classes are mutually exclusive.\n",
    "\n",
    "- Support Vector Machines (SVM)<br> SVMs can be used for multi-class classification using various strategies, including one-vs-one (OvO) and one-vs-rest (OvR). In OvO, a binary classifier is trained for each pair of classes, while in OvR, a binary classifier is trained for each class against the rest.\n",
    "\n",
    "#### Multi-Class Classification One-vs-Rest (OvR) or One-vs-All (OvA) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifier = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the Iris dataset, a well-known multi-class classification problem. We apply the OvR strategy by creating a Logistic Regression classifier with the multi_class='ovr' parameter. After training the model on the training data, we make predictions on the test data and evaluate its performance using accuracy and a classification report.\n",
    "\n",
    "This code demonstrates a straightforward use of the OvR strategy for multi-class classification in scikit-learn. The classifier builds multiple binary classifiers, one for each class, to handle the multi-class problem effectively.\n",
    "\n",
    "#### Multi-Class Classification Multinomial (Softmax) Logistic Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifier = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the Iris dataset, a classic multi-class classification problem. We apply Multinomial Logistic Regression by creating a Logistic Regression classifier with the multi_class='multinomial' parameter and specifying the solver as 'lbfgs', which is suitable for the multinomial loss. After training the model on the training data, we make predictions on the test data and evaluate its performance using accuracy and a classification report.\n",
    "\n",
    "This code demonstrates the use of Multinomial Logistic Regression for multi-class classification in scikit-learn, where the model directly estimates class probabilities using a softmax function, making it suitable for problems with mutually exclusive classes.\n",
    "\n",
    "#### Multi-Class Classification Support Vector Machines (SVM) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifier = SVC(kernel='linear', decision_function_shape='ovr')\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# Print a classification report\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we again use the Iris dataset, a classic multi-class classification problem. We apply Support Vector Machines (SVM) by creating an SVM classifier with the SVC class. We specify a linear kernel and use the \"one-vs-rest\" ('ovr') strategy for multi-class classification. After training the model on the training data, we make predictions on the test data and evaluate its performance using accuracy and a classification report.\n",
    "\n",
    "This code demonstrates a straightforward use of SVM for multi-class classification in scikit-learn, where the classifier constructs a hyperplane for each class to separate it from the rest.\n",
    "\n",
    "### 18.2 Multi-Output Classification/Regression\n",
    "\n",
    "Multi-output problems involve predicting multiple target variables (outputs) for each data point. Scikit-learn provides extensions to various algorithms for multi-output classification and regression tasks:\n",
    "\n",
    "- Multi-Output Regression<br> This extends standard regression to predict multiple continuous target variables simultaneously. For example, in a multi-output regression problem, you might predict both the price and the age of a house based on its features.\n",
    "\n",
    "- Multi-Label Classification<br> In multi-label classification, each data point can belong to multiple classes or categories simultaneously. For instance, in document classification, a document might belong to several topics at once.\n",
    "\n",
    "####  Multi-Output Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=2, n_targets=3, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "regressor = MultiOutputRegressor(LinearRegression())\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error for Each Output:\")\n",
    "print(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nR-squared for Each Output:\")\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we generate synthetic multi-output regression data with three target variables. We use MultiOutputRegressor to create a multi-output regression model and use LinearRegression as the base estimator. After training the model on the training data, we make predictions on the test data.\n",
    "\n",
    "We calculate the mean squared error (MSE) and R-squared (coefficient of determination) for each target variable to assess the model's performance.\n",
    "\n",
    "This code demonstrates a basic example of multi-output regression in scikit-learn, where the model simultaneously predicts multiple continuous target variables for each input data point.\n",
    "\n",
    "#### Multi-Output Classification/Regression Multi-Label Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples=100, n_features=5, n_classes=3, n_labels=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifier = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we generate synthetic multi-label classification data with three classes and two labels for each data point. We use MultiOutputClassifier to create a multi-label classification model and use RandomForestClassifier as the base estimator. After training the model on the training data, we make predictions on the test data.\n",
    "\n",
    "We calculate the accuracy for each label and print a classification report that includes precision, recall, F1-score, and support metrics for each label.\n",
    "\n",
    "This code demonstrates multi-label classification in scikit-learn, where the model predicts multiple binary labels for each input data point, allowing for scenarios where each data point can belong to multiple classes simultaneously.\n",
    "\n",
    "### 18.3 Summary\n",
    "\n",
    "Multi-class and multi-output algorithms are essential components of machine learning, addressing diverse and complex problems beyond binary classification or single-target regression. Here's a summary:\n",
    "\n",
    "Multi-class classification deals with categorizing data into more than two distinct classes or categories. Common strategies include One-vs-Rest (OvR), multinomial logistic regression, and support vector machines (SVM) with one-vs-one or one-vs-rest approaches. Ensemble methods like Random Forest and Gradient Boosting can be extended for multi-class classification.\n",
    "\n",
    "Multi-output problems involve predicting multiple target variables (outputs) for each data point, making them useful for tasks like multi-target regression and multi-label classification. Scikit-learn offers specialized tools like MultiOutputRegressor and MultiOutputClassifier to extend single-output algorithms for multi-output tasks.\n",
    "Multi-label classification is a subcategory where each data point can belong to multiple classes simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
