{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 17: Random Forests\n",
    "\n",
    "In this section, we will explore Random Forest, an ensemble learning method used for both classification and regression tasks.\n",
    "\n",
    "### 17.1 Understanding Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method used for both classification and regression tasks. Ensemble learning mean that it combines the predictions of multiple machine learning models (decision trees) to make more accurate predictions than individual models. Random Forest is based on decision trees, which are simple yet powerful models. Each decision tree makes predictions by recursively splitting the data based on feature values. It builds multiple decision trees during training and combines their predictions to improve accuracy and reduce overfitting.\n",
    "\n",
    "### 17.2 Training and Evaluation\n",
    "\n",
    "During training, Random Forest uses a technique called bootstrapping. It creates multiple subsets of the training data by randomly sampling with replacement. Each subset is used to train a separate decision tree. Random Forest selects a random subset of features to consider at each split. This randomness helps to reduce correlation among trees and improves generalization.\n",
    "\n",
    "For classification tasks, each tree in the forest makes a prediction, and the final prediction is determined by majority voting. For regression tasks, the predictions are averaged to obtain the final result.\n",
    "\n",
    "Random Forest can also provide a measure of feature importance, indicating which features are most influential in making predictions.\n",
    "\n",
    "Training individual decision trees in a Random Forest can be parallelized, making it efficient for large datasets.\n",
    "\n",
    "Random Forest has several hyperparameters to tune, such as the number of trees in the forest, the maximum depth of each tree, and the number of features to consider at each split.\n",
    "\n",
    "#### Random Forest Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(range(len(feature_importances)), feature_importances, tick_label=iris.feature_names)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates the use of a Random Forest Classifier to classify iris flower species based on their features. It splits the dataset into training and testing sets, then trains the classifier with 100 decision trees. After making predictions on the test data, it calculates the accuracy. Additionally, it visualizes the feature importances, showcasing which features contribute most to the classification. Overall, the Random Forest Classifier achieves high accuracy in classifying iris species and identifies the most important features in the dataset.\n",
    "\n",
    "#### Random Forest Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.linspace(0, 10, 100)  # Create 100 data points between 0 and 10\n",
    "y = 2 * X + 1 + np.random.normal(0, 1, 100)  # Generate y-values with noise\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],         # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],       # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],      # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]         # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best Random Forest Regressor model\n",
    "best_regressor = grid_search.best_estimator_\n",
    "\n",
    "# Predict target values\n",
    "y_pred = best_regressor.predict(X.reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# Plot the synthetic dataset and predictions\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X, y, label=\"Synthetic Data\", c='b', s=20)\n",
    "plt.plot(X, y_pred, c='r', label=\"Predictions\", linewidth=2)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Dataset and Predictions\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a synthetic dataset is created with a linear relationship between X and y, contaminated with random noise. A Random Forest Regressor is trained to predict y from X, and a hyperparameter grid search is performed to find the best model configuration. After training, the model's predictions are evaluated using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R²) metrics. The best model accurately captures the underlying linear relationship, and the evaluation metrics demonstrate its effectiveness. The synthetic dataset and model predictions are visualized, illustrating the model's ability to fit the data.\n",
    "\n",
    "### 17.3 Summary\n",
    "\n",
    "Random Forest is an ensemble learning method widely used in machine learning for both classification and regression tasks. It operates by constructing multiple decision trees during training and combines their outputs to make more robust predictions. Each tree is built using a random subset of the data and a random subset of features, reducing overfitting. In classification tasks, it selects the most frequent class among the trees (majority voting), while in regression tasks, it averages the outputs. Random Forests are known for their excellent performance, handling of high-dimensional data, and robustness against overfitting. They also provide feature importances, aiding in feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
