{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 10: Support Vector Machines (SVM)\n",
    "\n",
    "In this section, we will explore Support Vector Machines (SVM), a powerful supervised learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "### 10.1 Understanding Support Vector Machines\n",
    "\n",
    "Support Vector Machines work by find the best hyperplane (decision boundary) to separate data points of different classes. The hyperplane is selected such that the margin between the classes is maximized. Support vectors are the data points that lie closest to the decision boundary.\n",
    "\n",
    "In a two-dimensional space (with two features), a hyperplane is a line that separates data points of different classes. In higher dimensions, a plane becomes a hyperplane. SVM aims to find the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class. \n",
    "\n",
    "SVM can handle nonlinear relationships by using kernel functions. A kernel function transforms the original feature space into a higher-dimensional space where data points become more separable. Common kernels include the linear kernel (for linearly separable data), polynomial kernel, and radial basis function (RBF) kernel. The choice of the kernel function (e.g., linear, polynomial, RBF) depends on the characteristics of the data and the desired decision boundary shape.\n",
    "\n",
    "In real-world scenarios, data might not be perfectly separable. SVM introduces a soft margin that allows some misclassification to improve generalization. The C parameter controls the trade-off between maximizing the margin and minimizing the misclassification of training data. Smaller C values allow more misclassification (larger margin), while larger C values try to classify all points correctly (smaller margin).\n",
    "\n",
    "### 10.2 Support Vector Classifier (SVC) and Support Vector Regressor (SVR)\n",
    "\n",
    "SVC is a specific implementation of SVM for classification tasks. SVC aims to find a hyperplane that maximizes the margin between different classes while correctly classifying as many data points as possible. The classifier considers only the support vectors, which are the data points closest to the hyperplane. These support vectors determine the hyperplane's position and orientation. The output of an SVC is a predicted class label for each data point. Given a new set of input features, the SVC will assign a class label to each data point based on the learned decision boundary. The predicted class labels can be used for various purposes, such as making predictions, evaluating the model's performance, and understanding the separation of different classes.\n",
    "\n",
    "Support Vector Regressor (SVR) is another variant of the SVM algorithm, designed for regression tasks instead of classification. SVR aims to find a hyperplane that minimizes the margin between predicted and actual values while satisfying a specified margin of error (epsilon tube). Similar to SVC, SVR also uses support vectors to determine the hyperplane's position and orientation. The output of an SVR is a predicted target value for each data point. In regression tasks, SVR aims to predict continuous numerical values. Given a set of input features, the SVR will predict the target value for each data point. The predicted target values can be used to assess the model's accuracy in terms of predicting numerical outcomes.\n",
    "\n",
    "In summary, while Support Vector Classifier (SVC) is used for binary classification tasks, Support Vector Regressor (SVR) is used for regression tasks. Both algorithms utilize support vectors to find optimal hyperplanes that best fit the data points while considering the margin of tolerance and regularization.\n",
    "\n",
    "### 10.3 Training and Evaluation\n",
    "\n",
    "To train an SVM model, we need a labeled dataset with the target variable and the corresponding feature values. The model learns the optimal hyperplane or decision boundary that separates the classes or best fits the regression data.\n",
    "\n",
    "Once trained, we can evaluate the model's performance using evaluation metrics suitable for classification or regression tasks.\n",
    "\n",
    "For classification tasks (SVC), you can use accuracy, precision, recall, sensitivity, true Positive Rate), F1-Score, Confusion Matrix...\n",
    "\n",
    "For regression tasks (SVR), you can use mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), r-squared (Coefficient of Determination), residual plots...\n",
    "\n",
    "Both for SVC and SVR, it's crucial to perform cross-validation. This involves splitting the data into multiple folds, training the model on some folds, and testing on others. Cross-validation helps assess the model's generalization performance and minimizes overfitting. For both types of models, you might need to perform hyperparameter tuning. Use techniques like grid search to search through different combinations of hyperparameters and select the ones that yield the best results on cross-validation data.\n",
    "\n",
    "#### Example of SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=400, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1,\n",
    "                           random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM model with a linear kernel\n",
    "svm = SVC(kernel='linear')\n",
    "# Train the model on the training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate test set metrics\n",
    "y_pred_test = svm.predict(X_test_scaled)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "# Print test set metrics\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1-score:\", f1_test)\n",
    "\n",
    "# Extract coefficients and intercept from the SVM model\n",
    "coef = svm.coef_[0]\n",
    "intercept = svm.intercept_[0]\n",
    "# Calculate the slope and intercept of the decision boundary line\n",
    "slope = -coef[0] / coef[1]\n",
    "intercept_boundary = -intercept / coef[1]\n",
    "\n",
    "# Plot the decision boundaries, data points, support vectors, and SVM line\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Plot for training points\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', edgecolors='k', label='Train Data')\n",
    "# Plot the SVM decision boundary line\n",
    "plt.plot([min(X[:, 0]), max(X[:, 0])], \n",
    "         [slope * min(X[:, 0]) + intercept_boundary, slope * max(X[:, 0]) + intercept_boundary],\n",
    "         color='black', linestyle='dashed', linewidth=2, label='SVM Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('SVC Decision Boundaries - Train Data')\n",
    "\n",
    "# Plot for testing points\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolors='k', label='Test Data')\n",
    "# Plot the SVM decision boundary line\n",
    "plt.plot([min(X[:, 0]), max(X[:, 0])], \n",
    "         [slope * min(X[:, 0]) + intercept_boundary, slope * max(X[:, 0]) + intercept_boundary],\n",
    "         color='black', linestyle='dashed', linewidth=2, label='SVM Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('SVC Decision Boundaries - Test Data')\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, n_informative=1,\n",
    "                       noise=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVR model with a linear kernel and specified epsilon value\n",
    "epsilon_value = 20\n",
    "svr = SVR(kernel='linear', epsilon=epsilon_value)\n",
    "# Train the model on the training data\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict target values for training and testing data\n",
    "y_pred_train = svr.predict(X_train_scaled)\n",
    "y_pred_test = svr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "# Print evaluation metrics\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae_test)\n",
    "print(\"Mean Squared Error:\", mse_test)\n",
    "print(\"R-squared:\", r2_test)\n",
    "\n",
    "# Plot the SVR regression line, data points, and epsilon tube for training and testing data\n",
    "plt.figure(figsize=(12, 5))\n",
    "# Plot for training points\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='blue', label='Train Data')\n",
    "plt.plot(X_train, y_pred_train, color='green', linewidth=2, label='SVR Regression Line')\n",
    "plt.fill_between(X_train[:, 0], y_pred_train - epsilon_value, y_pred_train + epsilon_value,\n",
    "                 color='gray', alpha=0.3, label='Epsilon Tube')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('SVR with Epsilon Tube - Train Data')\n",
    "\n",
    "# Plot for testing points\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_train, y_pred_train, color='green', linewidth=2, label='SVR Regression Line')\n",
    "plt.fill_between(X_train[:, 0], y_pred_train - epsilon_value, y_pred_train + epsilon_value,\n",
    "                 color='gray', alpha=0.3, label='Epsilon Tube')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('SVR with Epsilon Tube - Test Data')\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we generate synthetic regression data and fit an SVR model to it. We evaluate the model using Mean Absolute Error, Mean Squared Error, and R-squared metrics. Finally, we visualize the SVR regression line, data points, and how well the SVR model fits the data.\n",
    "\n",
    "### 10.4 Conclusion\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning algorithms for classification and regression tasks. SVM aims to find the optimal hyperplane that separates the classes or best fits the regression data. Scikit-Learn provides the necessary classes to implement SVM easily. Understanding the concepts, training, and evaluation techniques are crucial for effectively using SVM in practice.\n",
    "\n",
    "In the next part, we will explore Naive Bayes classifiers, a family of probabilistic classifiers commonly used for classification tasks.\n",
    "\n",
    "Feel free to practice implementing Support Vector Machines (SVM) using Scikit-Learn. Experiment with different kernels, hyperparameter settings, and evaluation metrics to gain a deeper understanding of the algorithm and its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
