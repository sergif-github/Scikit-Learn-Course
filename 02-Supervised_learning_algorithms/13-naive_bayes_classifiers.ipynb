{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 13: Naive Bayes Classifiers\n",
    "\n",
    "In this section, we will explore Naive Bayes classifiers, a family of simple yet powerful supervised learning algorithms based on Bayes' theorem. Naive Bayes classifiers are widely used for classification tasks and are particularly effective when dealing with high-dimensional data.\n",
    "\n",
    "### 13.1 Understanding Naive Bayes Classifiers\n",
    "\n",
    "Naive Bayes classifiers are probabilistic models that use Bayes' theorem to make predictions. They assume that the features are conditionally independent of each other given the class label. This assumption simplifies the computation and makes Naive Bayes classifiers computationally efficient.\n",
    "\n",
    "Naive Bayes classifiers calculate the probability of each class label given the observed feature values and select the label with the highest probability as the predicted class.\n",
    "\n",
    "The assumption of feature independence is a key assumption in Naive Bayes classifiers. Although this assumption may not hold in all datasets, Naive Bayes classifiers can still perform well in practice, especially with large amounts of training data.\n",
    "\n",
    "When dealing with imbalanced datasets, Naive Bayes classifiers may produce biased models.\n",
    "\n",
    "The three main variants of Naive Bayes classifiers are:\n",
    "- Gaussian Naive Bayes: Assumes continuous features follow a Gaussian distribution. Suitable for real-valued data.\n",
    "- Multinomial Naive Bayes: Designed for discrete data, such as text documents with word counts. It models the probability of observing specific counts of discrete features.\n",
    "- Bernoulli Naive Bayes: Suited for binary or Boolean features, where each feature is a binary variable, representing presence or absence.\n",
    "\n",
    "It is important to choose the appropriate Naive Bayes classifier based on the nature of the features in your dataset.\n",
    "\n",
    "### 13.2 Training and Evaluation\n",
    "\n",
    "To train a Naive Bayes classifier, we need a labeled dataset with the target variable and the corresponding feature values. The model learns the probabilities of the feature values given each class label from the training data.\n",
    "\n",
    "Once trained, we can evaluate the model's performance using evaluation metrics suitable for classification tasks, such as accuracy, precision, recall, F1-score, or area under the ROC curve (AUC-ROC).\n",
    "\n",
    "#### Gaussian Naive Bayes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(0)\n",
    "class_samples = 100\n",
    "class1_attributes = np.random.randn(class_samples, 3) + np.array([2, 2, 2])\n",
    "class1_labels = np.zeros(class_samples)\n",
    "class2_attributes = np.random.randn(class_samples, 3) + np.array([4, 4, 4])\n",
    "class2_labels = np.ones(class_samples)\n",
    "class3_attributes = np.random.randn(class_samples, 3) + np.array([6, 6, 6])\n",
    "class3_labels = 2 * np.ones(class_samples)\n",
    "\n",
    "# Combine the classes\n",
    "X = np.vstack((class1_attributes, class2_attributes, class3_attributes))\n",
    "y = np.hstack((class1_labels, class2_labels, class3_labels))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the custom dataset and predictions\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=y_test, cmap=plt.cm.Set1, edgecolor='k')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_zlabel('Feature 3')\n",
    "ax1.set_title('Original Test Data')\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=y_pred, cmap=plt.cm.Set1, edgecolor='k')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.set_zlabel('Feature 3')\n",
    "ax2.set_title('Test Data with Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a custom dataset was generated with three classes, each exhibiting distinct clusters of three continuous attributes. The dataset was split into training and testing sets. A Gaussian Naive Bayes classifier was trained on the training data and used to predict class labels for the test data. The classifier achieved an accuracy of 92%, indicating its ability to correctly classify data points into the appropriate classes. The 3D scatter plots visualize the original test data and the corresponding predictions. Despite the simplicity of the Gaussian Naive Bayes algorithm, it effectively handled the multi-class, continuous attribute dataset, demonstrating its suitability for classification tasks involving continuous data with multiple classes.\n",
    "\n",
    "#### Multinomial Naive Bayes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"this is a positive sentence\",\n",
    "    \"negative sentiment in this text\",\n",
    "    \"we have a neutral document here\",\n",
    "    \"positive feedback is always appreciated\",\n",
    "    \"negative comments are not welcome\",\n",
    "    \"neutral statements are neither good nor bad\"\n",
    "]\n",
    "labels = [1, 0, 2, 1, 0, 2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we start with a collection of text documents and their corresponding labels. We split the data into training and testing sets, then use the CountVectorizer to convert the text data into numerical features. Finally, we train a Multinomial Naive Bayes classifier on the training data and evaluate its accuracy on the test data.\n",
    "\n",
    "This example demonstrates how to use Multinomial Naive Bayes for text classification tasks, where the input features are counts of words or other discrete items.\n",
    "\n",
    "#### Bernoulli Naive Bayes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert the feature values to binary using a threshold\n",
    "X_train_binary = (X_train > X_train.mean(axis=0)).astype(int)\n",
    "X_test_binary = (X_test > X_train.mean(axis=0)).astype(int)\n",
    "\n",
    "# Train a Bernoulli Naive Bayes classifier\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_binary, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_binary)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the \"wine\" dataset, which contains features related to the chemical composition of wines, and we perform binary conversion of feature values based on a threshold. Then, we train a Bernoulli Naive Bayes classifier to predict the wine classes based on the binary feature values and calculate the classifier's accuracy on the test set.\n",
    "\n",
    "### 13.4 Summary\n",
    "\n",
    "Naive Bayes classifiers are a family of probabilistic machine learning algorithms commonly used for classification tasks. They are based on Bayes' theorem and the \"naive\" assumption of feature independence, which simplifies calculations. Despite this simplification, they often perform surprisingly well in practice, especially when dealing with high-dimensional datasets.\n",
    "\n",
    "Naive Bayes classifiers are efficient, have low computational requirements, and work well even with limited training data. They find applications in spam detection, sentiment analysis, document classification, and more, where they can provide competitive accuracy with minimal tuning. However, the \"naive\" independence assumption may not hold in all real-world datasets, impacting their performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
