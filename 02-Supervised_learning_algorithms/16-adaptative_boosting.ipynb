{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Part 16: AdaBoost (Adaptive Boosting)\n",
    "\n",
    "In this section, we will explore AdaBoost (Adaptive Boosting), a popular ensemble learning algorithm used for classification tasks. AdaBoost combines multiple weak learners to create a strong predictive model.\n",
    "\n",
    "### 16.1 Understanding AdaBoost\n",
    "\n",
    "AdaBoost is an ensemble learning technique that combines multiple weak learners (often decision trees) to create a strong predictive model. AdaBoost assigns higher weights to misclassified instances, allowing subsequent weak learners to focus on those instances and improve overall prediction accuracy.\n",
    "\n",
    "The idea behind AdaBoost is to iteratively train weak learners on different subsets of the training data, with each weak learner giving more importance to the misclassified instances from the previous iterations. The final model is an aggregation of the weak learners' predictions, weighted by their performance. Each new weak learner focuses on the instances that the previous learners misclassified, and it assigns higher weights to those instances, effectively \"boosting\" their importance in the next round of training.\n",
    "\n",
    "AdaBoost can provide insights into feature importance. By analyzing the contribution of each feature across the ensemble, we can identify the most influential features in the predictive model.\n",
    "\n",
    "### 16.2 Training and Evaluation\n",
    "\n",
    "To train an AdaBoost model, we need a labeled dataset with the target variable and the corresponding feature values. The model learns by iteratively training weak learners on different subsets of the training data.\n",
    "\n",
    "AdaBoost models have hyperparameters that can be tuned to improve performance. These include the number of weak learners (decision trees), the learning rate, and the maximum depth of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(adaboost_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_adaboost_classifier = grid_search.best_estimator_\n",
    "y_pred = best_adaboost_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a breast cancer classification task is performed using the AdaBoost ensemble method with Decision Tree classifiers as weak learners. The Breast Cancer dataset is loaded, split into training and testing sets, and a base classifier (Decision Tree with max depth 1) is created. AdaBoost is then applied to boost the performance of this base classifier.\n",
    "\n",
    "A grid search is conducted to find the best hyperparameters for AdaBoost, specifically the number of weak classifiers (n_estimators) and the learning rate. GridSearchCV is used to systematically evaluate different combinations of hyperparameters using 5-fold cross-validation on the training data.\n",
    "\n",
    "The best hyperparameters found are a learning rate of 1.0 and 150 weak classifiers. The best AdaBoost classifier model is then selected based on these hyperparameters.\n",
    "\n",
    "The model is evaluated on the test data, achieving an impressive accuracy of 98%. The classification report provides additional metrics such as precision, recall, and F1-score for both classes (0 and 1), demonstrating strong performance in identifying breast cancer cases (class 1) with high precision and recall.\n",
    "\n",
    "Overall, this example showcases the effectiveness of AdaBoost in enhancing the classification performance of a simple Decision Tree classifier on the Breast Cancer dataset, resulting in a highly accurate and reliable predictive model.\n",
    "\n",
    "### 16.3 Random Forests vs AdaBoost\n",
    "\n",
    "AdaBoost and Random Forests are both ensemble learning methods used in machine learning, but they are quite different in terms of their underlying principles and how they build and combine individual base models.\n",
    "\n",
    "Here are the key differences between AdaBoost and Random Forests:\n",
    "\n",
    "1. Base Learners<br>\n",
    "AdaBoost (Adaptive Boosting) primarily uses a sequence of weak learners that are trained sequentially. Each new weak learner focuses on the instances that the previous learners misclassified, and it assigns higher weights to those instances, effectively \"boosting\" their importance in the next round of training.<br> Random Forests, on the other hand, use a collection of decision trees that are trained independently and in parallel. Each tree is constructed by randomly sampling both data points and a subset of features. The results from all the trees are then aggregated to make predictions.\n",
    "\n",
    "2. Weighting of Instances<br>\n",
    "AdaBoost assigns different weights to training instances during each iteration to focus on the samples that were misclassified by previous learners. This adaptive weighting of instances is a key feature of AdaBoost.<br> In Random Forests, all training instances are treated equally during the construction of each tree. There is no specific emphasis on misclassified instances from previous trees.\n",
    "\n",
    "3. Combining Predictions<br>\n",
    "AdaBoost combines the predictions from all weak learners by giving more weight to those that perform better. It uses a weighted majority vote to make the final prediction.<br> Random Forests combine the predictions by averaging (for regression) or taking a majority vote (for classification) from all individual trees.\n",
    "\n",
    "4. Parallelization<br>\n",
    "AdaBoost is inherently sequential, as each new weak learner depends on the outcomes of previous learners.<br> Random Forests can be parallelized, as the individual trees are constructed independently. This makes Random Forests suitable for parallel and distributed computing.\n",
    "\n",
    "In summary, while both AdaBoost and Random Forests are ensemble methods that aim to improve predictive performance, they differ in how they create and combine base models. AdaBoost focuses on adapting to the most challenging data points during training, while Random Forests use parallelization and randomization to create diverse and robust base models. The choice between the two depends on the problem at hand and the characteristics of the data.\n",
    "\n",
    "### 16.4 Summary\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a powerful ensemble learning algorithm for classification tasks. It combines multiple weak learners to create a strong predictive model. Scikit-Learn provides the necessary classes to implement AdaBoost easily. Understanding the concepts, training, and evaluation techniques is crucial for effectively using AdaBoost in practice.\n",
    "\n",
    "In the next part, we will explore Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), popular linear classification algorithms.\n",
    "\n",
    "Feel free to practice implementing AdaBoost using Scikit-Learn. Experiment with different hyperparameter settings, evaluation metrics, and techniques to gain a deeper understanding of the algorithm and its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
