{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Part 12: K-Nearest Neighbors (KNN)\n",
    "\n",
    "The k-nearest neighbors (K-NN) algorithm can be used in both supervised and unsupervised learning settings. In this section, we will explore K-Nearest Neighbors (KNN), used for both classification and regression tasks. \n",
    "\n",
    "### 12.1 Understanding K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN makes predictions based on the similarity of feature values between data points.\n",
    "\n",
    "In supervised learning with K-NN, the algorithm is used for classification or regression tasks. In the classification setting, K-NN assigns a class label to an unlabeled data point based on the class labels of its k-nearest neighbors in the training dataset. In regression, it estimates a numerical value for the target variable based on the values of its k-nearest neighbors.\n",
    "\n",
    "In unsupervised learning, K-NN can be used for clustering. In this case, it groups data points based on their similarity to their k-nearest neighbors, without using any pre-defined class labels or target variables.\n",
    "\n",
    "### 12.2 K-NN Classification\n",
    "\n",
    "K-Nearest Neighbors (K-NN) Classification is a supervised machine learning algorithm used for classification tasks. It's based on the principle that data points with similar features tend to belong to the same class. \n",
    "\n",
    "It starts with a labeled dataset, where each data point is associated with a class label. This dataset is used for training and testing the K-NN classifier. K is a hyperparameter representing the number of nearest neighbors to consider when making a prediction.  A smaller K (e.g., 3 or 5) makes the model more sensitive to noise, while a larger K makes it smoother but might miss fine-grained patterns. We also need to choose a distance metric (e.g., Euclidean, Manhattan) to measure the similarity between data points. The choice of distance metric affects how \"closeness\" is defined.\n",
    "\n",
    "K-NN doesn't involve traditional model training like other algorithms. Instead, it memorizes the entire training dataset. When a prediction is needed, the algorithm finds the K-nearest neighbors in the training data and assigns the class label that is most frequent among those neighbors. This can be done using majority voting.\n",
    "\n",
    "K-NN performance can be visualized by plotting decision boundaries. In a 2D feature space, it's easy to visualize how the regions associated with different classes are separated by the K-NN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=500, centers=3, cluster_std=4, random_state=42)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize lists to store K values and corresponding accuracies\n",
    "k_values = []\n",
    "accuracies = []\n",
    "\n",
    "# Test K values from 1 to 20\n",
    "for k in range(1, 21):\n",
    "    # Create a K-NN classifier with the current K value\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)    \n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    # Predict the target values for the testing data\n",
    "    y_pred = knn.predict(X_test)    \n",
    "    # Calculate accuracy and store the results\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    k_values.append(k)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a plot to visualize MSE and R-squared vs. K values\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "# Plot the dataset with different colors for each cluster\n",
    "for label in np.unique(y_test):\n",
    "    plt.scatter(X_test[y_test == label, 0], X_test[y_test == label, 1], edgecolors='k', label=f'Cluster {label}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Test dataset')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "# Plot the dataset with different colors for each predicted label\n",
    "for label in np.unique(y_pred):\n",
    "    plt.scatter(X_test[y_pred == label, 0], X_test[y_pred == label, 1], edgecolors='k', label=f'Predicted {label}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Predicted for test dataset')\n",
    "plt.legend()\n",
    "\n",
    "# Create a plot to visualize accuracy vs. K values\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(k_values, accuracies, marker='o', linestyle='-')\n",
    "plt.title('Accuracy vs. K Value for K-NN Classifier (Breast Cancer Dataset)')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate a plot where the x-axis represents different K values, and the y-axis represents the corresponding accuracies. We can observe how accuracy changes as you vary the number of neighbors (K) in the K-NN classifier.\n",
    "\n",
    "### 12.3 K-NN Regression\n",
    "\n",
    "In K-Nearest Neighbors (K-NN) regression the model aims to predict continuous numeric values. K-NN Regression achieves this by finding the K nearest data points in the training dataset to the input query point and then using these neighbors' target values to predict the output value for the query point. Some implementations of K-NN Regression allow for weighted averaging, where closer neighbors have more influence on the prediction than farther neighbors. This is useful when some neighbors are more relevant than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(300, 1), axis=0)\n",
    "y = 25 * np.sin(X).ravel() + 25  # Adjusted the range to be between 0 and 50\n",
    "y += 5 * np.random.randn(300)  # Added noise with mean 0 and standard deviation 5\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Initialize lists to store K values and corresponding MSE and R-squared values\n",
    "k_values = []\n",
    "mse_values = []\n",
    "r2_values = []\n",
    "# Create subplots to visualize bias-variance tradeoff\n",
    "plt.figure(figsize=(12,4))\n",
    "for i, k in enumerate([1, 5, 10]):\n",
    "    # Create a K-NN Regressor with the current K value\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    # Fit the regressor to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    # Predict target values for the testing data\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    # Calculate R-squared (R²)\n",
    "    r2 = r2_score(y_test, y_pred)    \n",
    "    print(f\"K = {k}; MSE = {mse}\")\n",
    "    print(f\"K = {k}; R2 = {r2}\")\n",
    "    # Sort the data points based on X values\n",
    "    sorted_indices = np.argsort(X_test.ravel())\n",
    "    X_test_sorted = X_test[sorted_indices]\n",
    "    y_test_sorted = y_test[sorted_indices]\n",
    "    y_pred_sorted = y_pred[sorted_indices]\n",
    "    # Create a subplot for the current K value\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X_test_sorted, y_test_sorted, label='data', marker='o')\n",
    "    plt.plot(X_test_sorted, y_pred_sorted, color='red', linestyle='-')\n",
    "    plt.title(f'K-NN Regression (K={k})')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid(True)\n",
    "# Adjust subplot layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we generated a synthetic dataset with a sinusoidal distribution and added noise, ensuring that the y-values fall within the range of 0 to 50. We then applied K-Nearest Neighbors (K-NN) regression with varying values of K (1, 5, and 8) to observe the bias-variance tradeoff in regression.\n",
    "\n",
    "We can observe K-NN regression for different K values. Blue points show our test data and with a single red line we can see the predicted points connected.\n",
    "\n",
    "For K=1, the model exhibits high variance as it closely fits the training data points. Consequently, it captures the noise present in the data, resulting in a highly fluctuating prediction line. It struggles to generalize to unseen data, as indicated by the high Mean Squared Error (MSE) and lower R-squared (R²) value. This situation represents overfitting, where the model is too complex and captures noise rather than the underlying pattern.\n",
    "\n",
    "For K=5, the model exhibits high bias because it smooths out the data by considering a larger number of nearest neighbors. This results in a simplified, less flexible model. While it generalizes better to the test data as evidenced by the lower MSE and R² value.\n",
    "\n",
    "For K=10, we strike a balance between bias and variance. The model captures the underlying sinusoidal pattern without being overly influenced by noise. This results in a moderate level of bias and variance, as indicated by the MSE and R² values. K=10 represents a good compromise between model complexity and generalization performance.\n",
    "\n",
    "### 12.4 Summary\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a versatile and intuitive algorithm in supervised learning, suitable for both classification and regression tasks. In KNN, predictions are made based on the majority class (for classification) or the average of nearest neighbors' target values (for regression) within a specified neighborhood, typically defined by the 'K' parameter.\n",
    "\n",
    "For KNN classification, it assigns a new data point to the class most common among its 'K' nearest neighbors. KNN is non-parametric, which means it doesn't assume a specific functional form for the data distribution, making it robust to complex patterns and adaptable to various datasets. However, it can be sensitive to the choice of 'K' and is computationally expensive for large datasets.\n",
    "\n",
    "In KNN regression, it estimates a continuous target variable by averaging the values of the 'K' nearest neighbors. This approach is particularly useful when the underlying relationship between features and targets is non-linear or lacks a clear mathematical representation. However, like KNN classification, selecting the right 'K' is crucial to achieving optimal predictive performance.\n",
    "\n",
    "The choice of K in K-NN  directly impacts the bias-variance tradeoff. Smaller K values lead to high variance and low bias, often resulting in overfitting. Larger K values reduce variance but introduce higher bias, potentially resulting in underfitting. An optimal K value, balances bias and variance, yielding a model that generalizes well to unseen data while capturing the underlying pattern in the training data. The choice of K should be carefully considered to achieve the best tradeoff for a given dataset.\n",
    "\n",
    "Overall, KNN is a simple yet powerful algorithm that can serve as a baseline for many supervised learning tasks, but it requires careful parameter tuning and consideration of computational efficiency for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
