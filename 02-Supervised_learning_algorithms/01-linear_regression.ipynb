{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 1: Linear Regression\n",
    "\n",
    "In this part, we will explore Linear Regression, one of the fundamental supervised learning algorithms used for predicting continuous numeric values. Linear Regression models the relationship between independent variables (features) and a dependent variable (target) by fitting a linear equation to the data.\n",
    "\n",
    "### 1.1 Understanding Linear Regression\n",
    "\n",
    "Linear Regression assumes a linear relationship between the independent variables and the target variable. The equation of a simple linear regression model can be represented as:\n",
    "\n",
    "$y = b_0 + b_1 * x_1 + b_2 * x_2 + b_n * x_n$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ is the target variable\n",
    "- $x_1$, $x_2$, ..., xn are the independent variables (features)\n",
    "- $b_0$, $b_1$, $b_2$, $b_n$ are the coefficients (slopes) of the linear equation\n",
    "\n",
    "The goal of linear regression is to find the best-fit line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "Linear Regression makes certain assumptions about the data. It assumes that:\n",
    "\n",
    "- There is a linear relationship between the independent variables and the target variable.\n",
    "- There is no multicollinearity among the independent variables.\n",
    "- The residuals (the differences between the predicted and actual values) follow a normal distribution.\n",
    "- The residuals have constant variance (homoscedasticity).\n",
    "\n",
    "### 1.2 Training and Evaluation\n",
    "\n",
    "To train a Linear Regression model, we need a labeled dataset with the target variable and the corresponding feature values. The model learns the coefficients (b0, b1, b2, ..., bn) by minimizing the residual sum of squares (RSS) or the mean squared error (MSE) between the predicted and actual values.\n",
    "\n",
    "Once trained, we can evaluate the model's performance using evaluation metrics such as:\n",
    "\n",
    "- Mean Squared Error (MSE) is a common metric used to evaluate the performance of regression models. It measures the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "    $\\text{MSE} = \\frac{1}{n} * \\sum(y_{actual} - y_{predicted})^2$\n",
    "    \n",
    "- Mean Absolute Error (MAE) is a metric that measures the average absolute difference between the predicted and actual values. It is less sensitive to outliers compared to MSE.\n",
    "\n",
    "    $\\text{MAE} = \\frac{1}{n} * \\sum|y_{actual} - y_{predicted}|$\n",
    "\n",
    "- Root Mean Squared Error (RMSE) is the square root of the MSE. It is a popular metric as it is in the same unit as the target variable, making it easier to interpret.\n",
    "\n",
    "    $\\text{RMSE} = \\sqrt{MSE}$\n",
    "\n",
    "- R-squared (coefficient of determination)is another evaluation metric for regression models that measures the proportion of the variance in the target variable that is predictable from the independent variables. R2 values range from 0 to 1, where 0 indicates that the model explains none of the variance, and 1 indicates a perfect fit.\n",
    "\n",
    "    $\\text{R2} = 1 - (\\sum(y_{actual} - y_{predicted})^2) / (\\sum(y_{actual} - y_{mean})^2)$\n",
    "\n",
    "\n",
    "### 1.3 Implementing Linear Regression in Scikit-Learn\n",
    "\n",
    "#### Example 1: One independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "# 100 x values between 0 and 10\n",
    "X = np.linspace(0, 10, 100)  \n",
    "# 100 y values following a normal distribution with 0 mean and 2 std desviation\n",
    "y = 2 * X + 5 + np.random.normal(0, 2, 100)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "\n",
    "# Create a linear regression model\n",
    "regressor = LinearRegression()\n",
    "# Fit the model on the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "# Plot the original data points and the linear regression line\n",
    "plt.scatter(X_train, y_train, label='Training data', color='blue')\n",
    "plt.scatter(X_test, y_test, label='Test data', color='green')\n",
    "plt.scatter(X_test, y_pred, label='Predicted test data', color='red', marker='x')\n",
    "plt.plot(X, regressor.predict(X.reshape(-1, 1)), label='Linear regression line', color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression Example')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the scatter plot of the training and test data points in blue and green, respectively. The red 'x' markers represent the predicted values for the test data based on the trained linear regression model. The red line represents the linear regression line fitted to the entire X range using the trained model. The goal of the linear regression model is to learn a line that best fits the training data and predicts the target variable for new data points.\n",
    "\n",
    "The most common metrics used for linear regression evaluation are the Mean Squared Error (MSE) and the R-squared (R2) score:\n",
    "\n",
    "MSE: Quantifies the average squared difference between the predicted values and the actual values. \n",
    "- The range of MSE values depends on the scale of the target variable and the nature of the data. Generally, the MSE can take any non-negative value, where:\n",
    "- A smaller MSE (close to 0) indicates that the model's predictions are closer to the actual values, and therefore, the model performs better.\n",
    "- A larger MSE means the model's predictions are farther from the actual values and the model is less accurate.\n",
    "\n",
    "R2: Proportion of the variance in the dependent variable (target). It indicates how well the model fits the data and explains the variability of the target variable. For the R-squared (R2) score, we expect it to be as close to 1 as possible. \n",
    "- R2 = 0: The model does not explain any variance in the target variable, and it performs no better than predicting the mean of the target variable.\n",
    "- R2 = 1: The model perfectly explains all the variance in the target variable, and it makes perfect predictions.\n",
    "\n",
    "In this case, the MSE is 2.488168969160713, which means, on average, the squared difference between the predicted and actual values is around 2.49. Lower values of MSE are generally preferred, but an MSE of 2.49 is still relatively small, indicating that the model's predictions are reasonably close to the true values.<br>\n",
    "An R2 score of 0.9325280805909615 indicates that approximately 93.25% of the variance in the target variable is explained by the independent variables in the model. This is quite high, suggesting that the model is doing an excellent job of capturing the relationships between the features and the target.<br>\n",
    "Overall, this is a strong indication that the linear regression model is a good fit for the data and is making accurate predictions.\n",
    "<br><br>\n",
    "\n",
    "#### Example 2: Two independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data with two independent variables\n",
    "np.random.seed(42)\n",
    "# 100 x1 values between 0 and 10\n",
    "x1 = np.linspace(0, 10, 100)\n",
    "# 100 x2 values between -5 and 5\n",
    "x2 = np.linspace(-5, 5, 100)\n",
    "# Combine the two independent variables into a 2D array as X\n",
    "X = np.column_stack((x1, x2))\n",
    "# Generate corresponding y values following a normal distribution with 0 mean and 2 std deviation\n",
    "y = 2 * x1 + 3 * x2 + np.random.normal(0, 2, 100)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "regressor = LinearRegression()\n",
    "# Fit the model on the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "# Plot the actual Y values in 3D\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': '3d'}, figsize=(8,6))\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], y_test, c='green', marker='o', label='Actual values')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], y_pred, c='red', marker='x', label='Predicted values')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y')\n",
    "ax.set_title('Actual and Predicted Y values')\n",
    "ax.view_init(elev=20, azim=-75)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this updated code, we generate two independent variables (x1, x2) and combine them into a 2D array X. The target variable y is generated using a linear combination of these two variables, with added noise. We then split the data into training and test sets.\n",
    "\n",
    "Next, we create a linear regression model and fit it to the training data with multiple independent variables (X_train). We make predictions on the test data with multiple independent variables (X_test) and obtain corresponding predicted target values (y_pred).\n",
    "\n",
    "The plot shows the actual and predicted values of the target variable (Y) in a 3D space, with two independent variables (X1 and X2). The green points represent the actual Y values for the test set while the red \"x\" markers represent the predicted Y values for the same test set.\n",
    "\n",
    "In this case, the MSE is 2.4881689691607116, which means, on average, the squared difference between the predicted and actual values is around 2.49. Lower values of MSE are generally preferred, but an MSE of 2.49 is still relatively small, indicating that the model's predictions are reasonably close to the true values.<br>\n",
    "An R2 score of 0.9882143330718749 indicates that approximately 98.82% of the variance in the target variable is explained by the independent variables in the model. This is quite high, suggesting that the model is doing an excellent job of capturing the relationships between the features and the target.<br>\n",
    "Overall, this is a strong indication that the linear regression model is a good fit for the data and is making accurate predictions.\n",
    "\n",
    "#### Example 3: Multiple independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Preprocess the data - standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create a linear regression model\n",
    "regressor = LinearRegression()\n",
    "# Fit the model on the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "# Plot the residuals (differences between y_actual and y_predicted)\n",
    "residuals = y_test - y_pred\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# Plot the predicted values vs. actual values\n",
    "ax1.scatter(y_test, y_pred, color='b', alpha=0.6)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='r')\n",
    "ax1.set_xlabel('Predicted Values')\n",
    "ax1.set_ylabel('Actual Values')\n",
    "ax1.set_title('Predicted vs. Actual Values')\n",
    "ax1.grid(True)\n",
    "# Plot the residuals against the predicted values\n",
    "ax2.scatter(y_pred, residuals, color='b', alpha=0.6)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residual Plot: Residuals vs. Predicted Values')\n",
    "ax2.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The California housing dataset is loaded using fetch_california_housing from scikit-learn. It contains features related to the housing properties and their corresponding target variable (median house value). The features are standardized using StandardScaler from scikit-learn to ensure that all features have zero mean and unit variance. The data is split into training and test sets using train_test_split from scikit-learn. The linear regression model is trained on the training data using fit method.\n",
    "\n",
    "In this example we cannot make a 2D or 3D plot because we have now multiple independent variables. Instead, we can plot the residuals.\n",
    "\n",
    "- In the first plot, actual vs. predicted values, we expect to have a strong correlation. Instead we see that model’s predictions aren’t very good at all. \n",
    "\n",
    "- In the second plot, predicted vs. residuals, positive values for the residuals (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct. Ideally, we want to see more points clustered around y=0.\n",
    "\n",
    "Finally the MSE value of 0.5558915986952444 suggests that, on average, the squared difference between the predicted and actual values is approximately 0.56. This indicates that the model's predictions are reasonably close to the true target values in the test set. The R-squared value of 0.5757877060324508 means that the regression model can explain approximately 57.58% of the variance in the target variable. <br> Overall, the model's performance seems to be moderate, as indicated by the MSE and R-squared values.\n",
    "\n",
    "### 1.6 Summary\n",
    "\n",
    "Linear Regression is a widely used supervised learning algorithm for modeling the relationship between a dependent variable (target) and one or more independent variables (features). It assumes a linear relationship between the features and the target, and aims to find the best-fit line that minimizes the difference between the predicted and actual values.\n",
    "\n",
    "To evaluate a Linear regression model:\n",
    "- Mean Squared Error (MSE) measures the average squared difference between the predicted values and the actual values.\n",
    "- Mean Absolute Error (MAE) measures the average absolute difference between the predicted and actual values. It is less sensitive to outliers compared to MSE.\n",
    "- Root Mean Squared Error (RMSE) is the square root of the MSE. It is a popular metric as it is in the same unit as the target variable, making it easier to interpret.\n",
    "- R-squared (R2), also known as the coefficient of determination, measures the proportion of the variance in the target variable that is predictable from the independent variables. R2 values range from 0 to 1, where 0 indicates that the model explains none of the variance, and 1 indicates a perfect fit.\n",
    "\n",
    "In summary MSE and RMSE penalize larger errors more severely, while MAE is more robust to outliers. R2 provides a measure of how well the model fits the data and can range from 0 to 1.\n",
    "\n",
    "To evaluate a Linear regression model graphically we can also see the residuals:\n",
    "For evaluate the performance of the regression model we can also see the actual vs. predicted plot and the predicted vs. residuals plot.\n",
    "\n",
    "- Actual vs. Predicted Plot:<br>\n",
    "    The actual vs. predicted plot is a scatter plot where the actual target values are plotted on the y-axis, and the corresponding predicted values are plotted on the x-axis. Each data point represents a specific sample in the test dataset. The plot helps to visualize how well the model predictions align with the actual values. Ideally, the points should lie close to the diagonal line (y = x), indicating a good fit of the model. Deviations from the diagonal line suggest discrepancies between the predicted and actual values.\n",
    "\n",
    "- Predicted vs. Residuals Plot:<br>\n",
    "    The predicted vs. residuals plot is also a scatter plot, where the predicted target values are plotted on the x-axis, and the residuals (differences between actual and predicted values) are plotted on the y-axis. This plot allows us to examine the relationship between the residuals and the predicted values. In an effective model, the residuals should be randomly scattered around the horizontal line at y = 0, indicating that the model has captured the underlying patterns in the data. Patterns or trends in the residuals indicate that the model is not performing optimally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
