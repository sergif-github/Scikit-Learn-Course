{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 2: Supervised Learning Algorithms\n",
    "\n",
    "### Part 6: Polynomial Regression\n",
    "\n",
    "In this part, we will explore Polynomial regression, a type of regression that extends the linear regression model to capture non-linear relationships.\n",
    "\n",
    "### 6.1 Understanding Polynomial regression\n",
    "\n",
    "Polynomial regression is a type of regression that extends the linear regression model to capture non-linear relationships between the independent and dependent variables. It achieves this by introducing polynomial terms (i.e., higher-order features) to the data, allowing the model to fit more complex patterns.\n",
    "\n",
    "In Scikit-Learn, polynomial regression can be easily implemented using the PolynomialFeatures transformer and a linear regression model. The PolynomialFeatures transformer is used to create polynomial features from the original features, and then these expanded features are used to fit a linear regression model.\n",
    "\n",
    "In simple terms, polynomial features are additional features that are derived from the original feature by raising it to various powers (e.g., square, cube) and also by multiplying different combinations of features together. This process allows us to capture non-linear relationships between the input feature X and the target variable y.\n",
    "\n",
    "For example, setting the degree to 3 means we will generate features up to X^3. If we have X = [x1], with degree=3, the PolynomialFeatures will generate X_poly = [1, x1, x1^2, x1^3].  The first feature (1) is the constant term, followed by x1 (original feature), x1^2 (square of the original feature), and x1^3 (cube of the original feature). \n",
    "\n",
    "These polynomial features are then used as input for the polynomial regression model, which allows the model to capture non-linear relationships between X and the target variable y.\n",
    "\n",
    "Advantages of Polynomial Regression\n",
    "- Flexibility: Polynomial regression can capture non-linear relationships between variables, making it more versatile than simple linear regression.\n",
    "- Improved Fit: By introducing polynomial terms, the model can better fit complex patterns in the data, potentially leading to higher predictive accuracy.\n",
    "- Interpretability: Although polynomial regression introduces more features, the coefficients can still provide insights into the relationships between variables.\n",
    "\n",
    "Disadvantages of Polynomial Regression\n",
    "- Overfitting: Higher-order polynomial terms can lead to overfitting, especially when dealing with a small dataset or a high degree of polynomial expansion.\n",
    "- Increased Complexity: The model complexity increases with higher-degree polynomials, making it more computationally expensive and harder to interpret.\n",
    "- Extrapolation: Polynomial regression is not suitable for extrapolating beyond the range of the training data, as it may produce unreliable predictions.\n",
    "\n",
    "### 6.2 Training end Evaluation\n",
    "\n",
    "Here's a step-by-step implementation of polynomial regression using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample data with a non-linear relationship\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.randn(100) * 0.1\n",
    "# Plot the original data points\n",
    "plt.scatter(X, y, s=50, label='Original Data')\n",
    "\n",
    "# Fit linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "# Make predictions\n",
    "y_pred = lin_reg.predict(X)\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"Linear Regression Mean Squared Error:\", mse)\n",
    "print(\"Linear Regression R-squared (R2) Score:\", r2)\n",
    "# Plot the linear regression line\n",
    "plt.plot(X, y_pred, color='red', label='Linear Regression')\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# Fit linear regression model\n",
    "lin_reg.fit(X_poly, y)\n",
    "# Make predictions\n",
    "y_pred = lin_reg.predict(X_poly)\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"\\nPolynomial Regression Mean Squared Error:\", mse)\n",
    "print(\"Polynomial Regression R-squared (R2) Score:\", r2)\n",
    "\n",
    "# Plot the polynomial regression line\n",
    "X_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "X_plot_poly = poly.transform(X_plot)\n",
    "y_pred_poly = lin_reg.predict(X_plot_poly)\n",
    "plt.plot(X, y_pred, color='blue', label='Polynomial Regression (Degree 3)')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression vs. Polynomial Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a sample dataset with a non-linear relationship between the input feature X and the target variable y. Specifically, the relationship follows a sine curve with some random noise added to the data. We first fit a linear regression model to the data. The linear regression line is represented by the red line on the plot. As expected, the linear regression line does not capture the non-linear nature of the data well. The mean squared error (MSE) and R-squared (R2) score are calculated to evaluate the performance of the linear regression model. To capture the non-linear relationship between X and y, we create polynomial features from the original feature X. In this case, we use a degree of 3 for the polynomial features. This means we generate polynomial features up to X^3, allowing the model to fit a cubic function to the data.\n",
    "\n",
    "By comparing the linear regression line (red) and the polynomial regression line (blue), we can clearly see how the polynomial regression better fits the data points, especially in regions with strong non-linear relationships. The polynomial regression line follows the general shape of the sine curve, providing a more accurate representation of the data.\n",
    "\n",
    "In summary, the polynomial regression (degree 3) outperforms the linear regression in this case, as it can capture the non-linear relationship more accurately, leading to a significantly lower MSE and a much higher R-squared score.\n",
    "\n",
    "### 6.3 Hyperparameter tunning\n",
    "\n",
    "In polynomial regression, the main hyperparameter to tune is the degree of the polynomial features. \n",
    "\n",
    "Overfitting is a common concern when using higher-degree polynomial features in polynomial regression. As the degree of the polynomial increases, the polynomial regression model can start to memorize the noise in the training data rather than capturing the underlying patterns. However, this increased flexibility comes at the cost of increased complexity. As a result, the model may perform very well on the training data but fail to generalize to new, unseen data. Overfitting.\n",
    "\n",
    "Consequently, the model becomes too specific to the training data and loses its ability to generalize to new data points.\n",
    "\n",
    "To avoid overfitting in polynomial regression, it's essential to find the right balance between model complexity (degree of the polynomial) and generalization. One common approach is to use techniques like cross-validation to tune the hyperparameters, such as the degree of the polynomial, and select the value that provides the best trade-off between bias and variance. By using cross-validation to evaluate the model's performance on different subsets of the data, you can identify the degree that achieves good performance on both the training and validation sets. This helps you choose a degree that is less prone to overfitting and can generalize better to new data.\n",
    "\n",
    "#### Example\n",
    "\n",
    "We are indeed changing the degree of the dataset features, and it's not a hyperparameter of the model itself. Using a for loop to simulate the grid search for different degrees is a more appropriate approach.\n",
    "\n",
    "Using cross-validation on the degree of polynomial features is possible, but it can be computationally expensive and time-consuming. The reason is that cross-validation involves splitting the data into multiple folds and training the model on each fold while validating on the remaining data. When searching for the best degree of polynomial features, this process would need to be repeated for each degree being tested.\n",
    "\n",
    "For each fold, the model would have to create polynomial features with the given degree, fit the regression model, and evaluate its performance. This process can be quite slow, especially when the dataset is large or the degree range is wide.\n",
    "\n",
    "Instead of using cross-validation directly on the degree, a more efficient approach is to use cross-validation to evaluate the performance of the polynomial regression model with pre-selected degrees. This way, you can avoid repeating the polynomial feature creation for each fold, significantly reducing computation time.\n",
    "\n",
    "In the following provided code, the for loop tests different degrees of polynomial features and calculates the mean squared error and R-squared score for each degree using cross-validation. This approach is a practical compromise to find the best degree without the excessive computational burden of using cross-validation directly on the degree parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample data with a non-linear relationship\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.randn(100) * 0.1\n",
    "# Plot the original data points\n",
    "plt.scatter(X, y, s=50, label='Original Data')\n",
    "\n",
    "# Define the list of degrees to test\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7]\n",
    "best_degree = None\n",
    "best_mse = float('inf')\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    # Fit linear regression model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_poly, y)\n",
    "    # Make predictions\n",
    "    y_pred = lin_reg.predict(X_poly)\n",
    "    # Calculate mean squared error and R-squared score\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    # Print the results for the current degree\n",
    "    if degree == 1:\n",
    "        print(\"Linear Regression\".format(degree))\n",
    "    else:\n",
    "        print(\"Polynomial Regression (Degree {})\".format(degree))\n",
    "    print(\"\\tMean Squared Error:\", mse)\n",
    "    print(\"\\tR-squared (R2) Score:\", r2)\n",
    "    # Plot the polynomial regression line for the current degree\n",
    "    X_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_pred_poly = lin_reg.predict(X_plot_poly)\n",
    "    if degree == 1:\n",
    "        plt.plot(X_plot, y_pred_poly, label='Linear Regression')\n",
    "    else:\n",
    "        plt.plot(X_plot, y_pred_poly, label='Polynomial Regression (Degree {})'.format(degree))\n",
    "    # Update the best degree if necessary\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_degree = degree\n",
    "\n",
    "# Create polynomial features with the best degree\n",
    "best_poly = PolynomialFeatures(degree=best_degree)\n",
    "X_poly = best_poly.fit_transform(X)\n",
    "# Fit the linear regression model with the best polynomial features\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "# Make predictions\n",
    "y_pred = lin_reg.predict(X_poly)\n",
    "# Calculate mean squared error and R-squared score\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"\\nBest Polynomial Regression (Degree {})\".format(degree))\n",
    "print(\"\\tMean Squared Error:\", mse)\n",
    "print(\"\\tR-squared (R2) Score:\", r2)\n",
    "\n",
    "# Plot the best polynomial regression line\n",
    "X_plot = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "X_plot_poly = best_poly.transform(X_plot)\n",
    "y_pred_poly = lin_reg.predict(X_plot_poly)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Polynomial Regression with Different Degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code performs polynomial regression on a sample dataset with a non-linear relationship. It tests different degrees of polynomial features using a for loop and plots the resulting polynomial regression lines. The mean squared error and R-squared score are calculated for each degree. The best degree with the lowest mean squared error is then selected.\n",
    "\n",
    "Hyperparameter tuning allows us to find the optimal complexity of the polynomial model, balancing between overfitting and underfitting. By selecting the best polynomial degree, we can build a polynomial regression model that generalizes well to unseen data and captures the underlying non-linear relationship in the data more accurately.\n",
    "\n",
    "### 6.4 Summary\n",
    "\n",
    "Overall, Polynomial regression is a valuable technique that extends linear regression to capture complex like non-linear data patterns.\n",
    "\n",
    "Remember that the choice of the polynomial degree (degree parameter in PolynomialFeatures) is essential to balance model complexity and overfitting.\n",
    "\n",
    "Overfitting is a concern in polynomial regression, especially as the degree of the polynomial increases. Cross-validation is a valuable tool to help find the optimal degree and prevent overfitting by selecting a model that performs well on unseen data. Experiment with different degrees and evaluate the model's performance to find the optimal degree for your specific dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
