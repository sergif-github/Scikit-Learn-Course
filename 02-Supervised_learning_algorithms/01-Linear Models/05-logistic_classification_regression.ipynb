{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Scikit-Learn\n",
    "\n",
    "## Section 3: Supervised Learning Algorithms\n",
    "\n",
    "### Part 5: Logistic Classification / Regression\n",
    "\n",
    "In this section, we will explore Logistic Regression, a popular supervised learning algorithm used for binary and multiclass classification tasks.\n",
    "\n",
    "### 5.1 Understanding Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm that uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of an instance belonging to a specific class. The logistic function converts the linear equation into a range between 0 and 1, representing the probability.\n",
    "\n",
    "Despite its name, logistic regression is primarily used for binary or multiclass classification problems. It models the probability that an instance belongs to a particular class, typically represented as 0 and 1, True and False, or Negative and Positive.\n",
    "\n",
    "It's similar to linear regression except logistic regression predicts when something is true or false instead of predicting something continuous. Instead of fitting a line, logistic regression fits a logistic function with values from 0 to 1. That means that the curve talks about the probability of beeing a particular class based on a value.\n",
    "\n",
    "With linear regression we fit a line using least squares, we find a line that minimizes the sum of the squares of the residuals. We also use the residuals to calculate $R^2$ and evaluate the model performance.\n",
    "\n",
    "The equation of the logistic regression model can be represented as:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{(1 + e^{-z})}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- Ïƒ(z) is the output (probability) of the function.\n",
    "- $z$ is the linear combination of the feature values and their corresponding model coefficients.\n",
    "\n",
    "With logistic regression we cannot use the concept of residuals and so we can't use the $R^2$ to evaluate the model performance. Instead we use something called maximum likehood.\n",
    "\n",
    "### 5.2 Training and Evaluation\n",
    "\n",
    "To train a logistic regression model, you need a labeled dataset, where the target variable (class label) is binary. The model is trained to maximize the likelihood of the observed class labels given the input features. The likelihood function measures how well the given model's parameters explain the observed data. The optimization process aims to find the coefficients that best separate the two classes. \n",
    "\n",
    "When we have found the best sigmoid function now ww can preduct the probabilities of belonging to a class. By default, most classifiers, including logistic regression, use a threshold of 0.5, meaning that a predicted probability greater than or equal to 0.5 is classified as the positive class, and a predicted probability less than 0.5 is classified as the negative class.\n",
    "\n",
    "Finally evaluate out model performance. This time the output is not continuous but categorical (classes). The concept of residuals, which are based on continuous predictions, doesn't directly apply in the same way as it does for regression. For classification models, it's more common to evaluate performance using metrics like accuracy, precision, recall, F1-score, and the ROC curve. These metrics provide a more intuitive understanding of how well the model is classifying data into different categories. Let's dive into each one:\n",
    "\n",
    "1. Confusion Matrix: A confusion matrix is a tabular representation that compares the predicted class labels with the actual class labels.\n",
    "\n",
    "    | Actual/Predicted | Positive | Negative |\n",
    "    |------------------|----------|----------|\n",
    "    | Positive         | True Positive (TP) | False Negative (FN) |\n",
    "    | Negative         | False Positive (FP) | True Negative (TN) |\n",
    "    \n",
    "    It breaks down the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "2. Accuracy: Accuracy is a fundamental metric that measures the proportion of correct predictions (both true positives and true negatives) out of all predictions. It calculates the ratio of correctly predicted instances to the total number of instances. Accuracy might not be sufficient for imbalanced datasets where one class is much more frequent than the other.\n",
    "<br/>$Accuracy = (TP + TN) / (TP + TN + FP + FN)$\n",
    "\n",
    "3. Precision: Precision mesures the proportion of true positive predictions out of all positive predictions made by the model. It answers the question: \"Of all instances predicted as positive, how many are actually positive?\" High precision indicates that when the model predicts a positive result, it's likely to be correct.\n",
    "<br/>$Precision = TP / (TP + FP)$\n",
    "\n",
    "4. Recall (Sensitivity or True Positive Rate): Recall measures the proportion of actual positive instances that were correctly predicted as positive. It answers the question: \"Of all actual positive instances, how many did the model predict as positive?\" High recall indicates that the model is good at identifying positive instances.<br/>$Recall = TP / (TP + FN)$\n",
    "\n",
    "5. F1-Score: The F1-score is the harmonic mean of precision and recall. It considers both false positives and false negatives, making it a balanced metric that is especially useful when class distribution is imbalanced.\n",
    "<br/>$F1-Score = 2 * (Precision * Recall) / (Precision + Recall)$\n",
    "\n",
    "6. Specificity (True Negative Rate): Specificity measures the proportion of actual negative instances that were correctly predicted as negative.\n",
    "<br/>$Specificity = TN / (TN + FP)$\n",
    "\n",
    "7. False Positive Rate: The false positive rate is the proportion of actual negative instances that were incorrectly predicted as positive.\n",
    "<br/>$False Positive Rate = FP / (FP + TN)$\n",
    "\n",
    "8. False Negative Rate: The false negative rate is the proportion of actual positive instances that were incorrectly predicted as negative.\n",
    "<br/>$False Negative Rate = FN / (FN + TP)$\n",
    "\n",
    "9. ROC Curve (Receiver Operating Characteristic Curve):\n",
    "The ROC curve is a graphical representation of the trade-off between true positive rate (recall) and false positive rate (1 - specificity) as the classification threshold varies. It helps assess the model's ability to distinguish between positive and negative classes. The area under the ROC curve (AUC) summarizes the performance across all possible thresholds.\n",
    "\n",
    "### 5.3 Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "# Generate random data from one class\n",
    "X, y = make_classification(n_samples=50, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "# Train the model on the train data\n",
    "logreg.fit(X_train, y_train)\n",
    "# Generate a range of feature values for prediction\n",
    "X_range = np.linspace(min(X), max(X), 50).reshape(-1, 1)\n",
    "# Predict the probability of belonging to class 1 for the range of feature values\n",
    "probabilities_train = logreg.predict_proba(X_range)[:, 1]  # Probability of belonging to class 1 for train data\n",
    "probabilities_test = logreg.predict_proba(X_range)[:, 1]   # Probability of belonging to class 1 for test data\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(18, 5))\n",
    "# Plot the original data points\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, c='blue', label='Data Points')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Class Label')\n",
    "plt.title('Original Data Points')\n",
    "plt.legend()\n",
    "# Plot sigmoid curve on train data along with predicted probabilities as red points\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_train, y_train, c='blue', label='Train Data')\n",
    "plt.plot(X_range, probabilities_train, color='purple', label='Sigmoid (Train)')\n",
    "plt.scatter(X_train, logreg.predict_proba(X_train)[:, 1], color='red', marker='o', label='Predicted Probabilities (Train)')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Probability / Class Label')\n",
    "plt.legend()\n",
    "plt.title('Sigmoid on Train Data')\n",
    "# Plot sigmoid curve on test data along with predicted probabilities as red points\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_test, y_test, c='blue', label='Test Data')\n",
    "plt.plot(X_range, probabilities_test, color='purple', label='Sigmoid (Test)')\n",
    "plt.scatter(X_test, logreg.predict_proba(X_test)[:, 1], color='red', marker='o', label='Predicted Probabilities (Test)')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Probability / Class Label')\n",
    "plt.legend()\n",
    "plt.title('Sigmoid on Test Data')\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Predict classes for train and test data\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "'''\n",
    "# Specify a custom threshold\n",
    "custom_threshold = 0.3\n",
    "# Predict classes for train and test data using the custom threshold\n",
    "y_pred_train = (logreg.predict_proba(X_train)[:, 1] >= custom_threshold).astype(int)\n",
    "y_pred_test = (logreg.predict_proba(X_test)[:, 1] >= custom_threshold).astype(int)\n",
    "'''\n",
    "# Display evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with feature values between -1.5 and 2.5. These features have associated a class label with values 0 or 1. We use a train-test split on our dataset using train_test_split from sklearn.model_selection. The left plot shows the training data points before fitting a logistic regresion. The middle plot displays the sigmoid curve fitted to the training data, and the the right plot shows the sigmoid curve found applied to the test data. \n",
    "\n",
    "The logistic regression model is trained on the training data and then used to predict the probabilities of belonging to class 0 on the test data. The predict_proba method returns an array where each row represents a data point, and the two columns represent the probabilities of belonging to class 0 and class 1, respectively. Now that we have the mapped probabilities of belonging to a class for each point, we specify a threshold to decide when we belong or not to a class based on our needs. By default, most classifiers, including logistic regression, use a threshold of 0.5, meaning that a predicted probability greater than or equal to 0.5 is classified as the positive class, and a predicted probability less than 0.5 is classified as the negative class.\n",
    "\n",
    "Using the default threshold of 0.5 we can finally evaluate out model performance.\n",
    "- An accuracy of 0.87 indicates that approximately 87% of the predictions were correct.\n",
    "- A precision of 0.8 indicates that 80% of the instances predicted as positive were actually positive.\n",
    "- A recall of 1.0 means that the model is able to identify all positive instances.\n",
    "- An F1-score of 0.89 indicates a good balance between precision and recall.\n",
    "\n",
    "### 5.4 Conclusion\n",
    "\n",
    "Logistic Regression is a widely used algorithm for binary and multiclass classification tasks. It models the probability of an instance belonging to a certain class based on the values of independent variables. Scikit-Learn provides the LogisticRegression class to implement logistic regression models easily. Understanding the underlying assumptions and techniques is crucial for interpreting the results and applying logistic regression effectively.\n",
    "\n",
    "In the next part, we will explore another popular supervised learning algorithm, Decision Trees, used for both classification and regression tasks.\n",
    "\n",
    "Feel free to practice implementing Logistic Regression using Scikit-Learn. Experiment with different features, evaluation metrics, and techniques to gain a deeper understanding of the algorithm and its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
