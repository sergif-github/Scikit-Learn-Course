{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Techniques in Scikit-Learn\n",
    "\n",
    "## Section 6: Model Evaluation and Selection\n",
    "\n",
    "### Part 3: Leave-One-Out (LOO) Cross-Validation\n",
    "\n",
    "In this part, we will explore Leave-One-Out (LOO) Cross-Validation, a special case of K-fold Cross-Validation where each fold contains only one sample as the testing set, and the rest are used for training. LOO Cross-Validation is particularly useful when dealing with small datasets and can provide an unbiased estimate of a model's performance. Let's dive in!\n",
    "\n",
    "### 3.1 Understanding Leave-One-Out (LOO) Cross-Validation\n",
    "\n",
    "In LOO Cross-Validation, each sample in the dataset is used as the testing set exactly once, while the remaining samples form the training set. This process is repeated for all samples in the dataset. As a result, LOO Cross-Validation creates as many folds as there are samples in the dataset.\n",
    "\n",
    "LOO Cross-Validation has the advantage of providing an unbiased estimate of a model's performance, especially when the dataset is small. However, it can be computationally expensive for large datasets due to the large number of iterations.\n",
    "\n",
    "### 3.2 Using Leave-One-Out (LOO) Cross-Validation in Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides the LeaveOneOut class, which can be used to perform LOO Cross-Validation. Here's an example of how to use it:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming X and y are the feature matrix and target vector, respectively\n",
    "loo = LeaveOneOut()\n",
    "clf = SVC(kernel='linear')\n",
    "scores = cross_val_score(clf, X, y, cv=loo)\n",
    "```\n",
    "\n",
    "In this example, LeaveOneOut creates as many folds as there are samples in the dataset. The cross_val_score function automatically handles the process of training and evaluating the model for each fold.\n",
    "\n",
    "### 3.3 Summary\n",
    "\n",
    "Leave-One-Out (LOO) Cross-Validation is a useful technique, especially for small datasets, as it provides an unbiased estimate of a model's performance. It can be computationally expensive for large datasets due to the high number of iterations. Scikit-Learn's LeaveOneOut class makes it easy to perform LOO Cross-Validation.\n",
    "\n",
    "In the next part, we will explore other evaluation and selection techniques commonly used in machine learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
