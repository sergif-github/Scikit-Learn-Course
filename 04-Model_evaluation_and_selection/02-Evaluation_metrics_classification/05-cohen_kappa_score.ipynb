{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Techniques in Scikit-Learn\n",
    "\n",
    "## Section 6: Model Evaluation and Selection\n",
    "\n",
    "### Part 5: Cohen's Kappa Score\n",
    "\n",
    "In this part, we will explore Cohen's Kappa Score, a widely used evaluation metric for measuring the agreement between two raters. Cohen's Kappa is especially useful when dealing with imbalanced datasets or when the accuracy alone may not provide an accurate representation of the model's performance. Understanding Cohen's Kappa Score is crucial for assessing inter-rater agreement in classification tasks. Let's dive in!\n",
    "\n",
    "### 5.1 Understanding Cohen's Kappa Score\n",
    "\n",
    "Cohen's Kappa is a metric used to evaluate the agreement between two raters (e.g., human annotators or different models). It measures the agreement beyond chance between the raters' predictions. The formula for Cohen's Kappa is as follows:\n",
    "\n",
    "$K = \\frac{P_0 - P_e}{1-P_e}$\n",
    "\n",
    "Where:\n",
    "- P_0 is the observed proportion of agreement between the two raters.\n",
    "- P_e is the proportion of agreement expected by chance.\n",
    "\n",
    "Cohen's Kappa ranges from -1 to 1. A value of -1 indicates complete disagreement between the raters, 0 indicates agreement equal to chance, and 1 indicates perfect agreement between the raters.\n",
    "\n",
    "### 5.2 Interpreting Cohen's Kappa Score\n",
    "\n",
    "Cohen's Kappa provides a measure of agreement that considers the possibility of agreement by chance. Higher values of Cohen's Kappa indicate better agreement between the raters, beyond what could be expected by chance. A value close to 1 indicates strong agreement, while a value close to 0 suggests little agreement beyond chance.\n",
    "\n",
    "### 5.3 Using Cohen's Kappa Score in Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides the cohen_kappa_score function to calculate Cohen's Kappa Score. Here's an example of how to use it:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y1 and y2 are the labels predicted by two raters or models\n",
    "kappa_score = cohen_kappa_score(y1, y2)\n",
    "```\n",
    "\n",
    "### 5.4 Summary\n",
    "\n",
    "Cohen's Kappa Score is a valuable evaluation metric for measuring the agreement between two raters or models in classification tasks. It takes into account the agreement beyond what could be expected by chance and is especially useful for imbalanced datasets. Scikit-Learn's cohen_kappa_score function allows easy computation of Cohen's Kappa Score.\n",
    "\n",
    "In the next part, we will explore other evaluation metrics commonly used in regression and classification tasks.\n",
    "\n",
    "Feel free to practice calculating Cohen's Kappa Score using Scikit-Learn's cohen_kappa_score function with different models or raters. Compare the scores to assess the agreement between the predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
