{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Techniques in Scikit-Learn\n",
    "\n",
    "## Section 6: Model Evaluation and Selection\n",
    "\n",
    "### Part 2: Precision, Recall, and F1-score\n",
    "\n",
    "In this part, we will explore three important evaluation metrics commonly used for classification models: Precision, Recall, and F1-score. These metrics provide a more comprehensive assessment of a model's performance, especially in the presence of imbalanced datasets. Understanding Precision, Recall, and F1-score is crucial for evaluating and comparing the effectiveness of classification models. Let's dive in!\n",
    "\n",
    "### 2.1 Understanding Precision, Recall, and F1-score\n",
    "\n",
    "Precision, Recall, and F1-score are metrics used to evaluate the performance of classification models. They are calculated based on the counts of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) predictions made by the model. These counts are used to compute the following metrics:\n",
    "\n",
    "Precision (also known as Positive Predictive Value) measures the proportion of true positive predictions among all positive predictions. \n",
    "It is defined as:\n",
    "\n",
    "$Precision = \\frac{TP+FP}{TP}$\n",
    "​\n",
    "Recall (also known as Sensitivity or True Positive Rate) measures the proportion of true positive predictions among all actual positive instances.\n",
    "It is defined as:\n",
    "\n",
    "$Recall = \\frac{TP+FN}{TP}$\n",
    "\n",
    "F1-score is the harmonic mean of Precision and Recall, providing a balance between the two metrics. \n",
    "It is defined as:\n",
    "\n",
    "$F1−score = \\frac{Precision+Recall}{2×Precision×Recall}$\n",
    "​\n",
    "### 2.2 Interpreting Precision, Recall, and F1-score\n",
    "\n",
    "Precision, Recall, and F1-score are particularly useful when dealing with imbalanced datasets, where one class has significantly fewer instances than the other. In such cases, Accuracy alone may not provide an accurate representation of the model's performance. High Precision indicates a low false positive rate, while high Recall indicates a low false negative rate. The F1-score combines both metrics, providing a single value that balances Precision and Recall.\n",
    "\n",
    "### 2.3 Using Precision, Recall, and F1-score in Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides the precision_recall_fscore_support function to calculate Precision, Recall, and F1-score simultaneously. Here's an example of how to use it:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels, respectively\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "```\n",
    "\n",
    "You can set the average parameter to 'binary' when dealing with binary classification tasks. For multi-class classification tasks, you can use other averaging options like 'macro', 'micro', or 'weighted'.\n",
    "\n",
    "### 2.4 Summary\n",
    "\n",
    "Precision, Recall, and F1-score are important evaluation metrics for classification models, especially when dealing with imbalanced datasets. They provide a more comprehensive assessment of the model's performance, taking into account both false positive and false negative rates. Scikit-Learn's precision_recall_fscore_support function allows easy computation of these metrics for classification tasks.\n",
    "\n",
    "In the next part, we will explore other evaluation metrics commonly used in regression and classification tasks.\n",
    "\n",
    "Feel free to practice calculating Precision, Recall, and F1-score using Scikit-Learn's precision_recall_fscore_support function with different classification models. Compare the metrics to assess the performance of the models on your dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
