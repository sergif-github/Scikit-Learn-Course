{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Techniques in Scikit-Learn\n",
    "\n",
    "## Section 6: Model Evaluation and Selection\n",
    "\n",
    "### Part 4: Log Loss\n",
    "\n",
    "In this part, we will explore the concept of Log Loss, a popular evaluation metric used for binary and multiclass classification models. Log Loss, also known as Cross-Entropy Loss, measures the accuracy of a model's predicted probabilities compared to the true target values. Understanding Log Loss is crucial for assessing the calibration and confidence of probabilistic classifiers. Let's dive in!\n",
    "\n",
    "### 4.1 Understanding Log Loss\n",
    "\n",
    "Log Loss is a metric used to evaluate the performance of classification models that predict probabilities. It measures the accuracy of the predicted probabilities compared to the true target values. The formula for Log Loss is as follows:\n",
    "\n",
    "$\\text{Log Loss} = -\\frac{1}{n}\\sum\\limits _{i=1} ^{n}[y_{true,i}\\log{(y_{pred,i})}+(1-y_{true,i})\\log{(1-y_{pred,i})}]$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of samples in the dataset.\n",
    "- $y_{true,i}$ is the true probability of the positive class (target value) for the i-th sample.\n",
    "- $y_{pred,i}$ is the predicted probability of the positive class for the i-th sample.\n",
    "\n",
    "Log Loss penalizes incorrect and uncertain predictions, encouraging the model to output well-calibrated probabilities.\n",
    "\n",
    "### 4.2 Interpreting Log Loss\n",
    "\n",
    "Log Loss ranges from 0 to positive infinity. A perfect classifier that perfectly calibrates probabilities will have a Log Loss of 0. A higher Log Loss indicates worse model calibration and less confident predictions.\n",
    "\n",
    "### 4.3 Using Log Loss in Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides the log_loss function to calculate Log Loss. Here's an example of how to use it:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Assuming y_true and y_prob are the true labels and predicted probabilities, respectively\n",
    "log_loss_value = log_loss(y_true, y_prob)\n",
    "```\n",
    "\n",
    "### 4.4 Summary\n",
    "\n",
    "Log Loss is an important evaluation metric for probabilistic classifiers, especially for binary and multiclass classification tasks. It assesses the accuracy and calibration of predicted probabilities compared to the true target values. A lower Log Loss indicates better-calibrated and more confident predictions. Scikit-Learn's log_loss function allows easy computation of Log Loss for classification tasks.\n",
    "\n",
    "In the next part, we will explore other evaluation metrics commonly used in regression and classification tasks.\n",
    "\n",
    "Feel free to practice calculating Log Loss using Scikit-Learn's log_loss function with different classification models. Compare the Log Loss values to assess the performance of the models on your dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
