{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 4: Feature selection\n",
    "\n",
    "In machine learning, feature selection is a crucial step in preparing our data for building effective models. It involves choosing the most relevant features from the original dataset to improve the model's performance and reduce computational complexity. Scikit-Learn provides various methods for feature selection, and two common techniques are SelectKBest and SelectPercentile.\n",
    "\n",
    "### Part 1: SelectKBest / SelectPercentile\n",
    "\n",
    "SelectKBest is a feature selection method that selects the top k features based on a specified scoring function. It is commonly used for classification tasks, where the f_classif score function is a popular choice. The f_classif function uses the ANOVA F-value to rank the features by their importance with respect to the target variable.\n",
    "\n",
    "### 1.1 Using SelectKBest\n",
    "\n",
    "Parameters:\n",
    "- score_func: The scoring function used to evaluate the features' importance. Common choices for classification tasks include f_classif (ANOVA F-value) and mutual_info_classif (mutual information).\n",
    "- k: The number of top features to select.\n",
    "\n",
    "Advantages:\n",
    "- Simple and straightforward to use.\n",
    "- Allows specifying the number of features to keep, which can be helpful when you have prior knowledge about the optimal feature count.\n",
    "\n",
    "Disadvantages:\n",
    "- Assumes that the selected features are independent, which might not always hold in real-world datasets.\n",
    "- Does not consider feature dependencies, which may lead to suboptimal results in certain scenarios.\n",
    "\n",
    "Here's how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "x, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "print(\"Original feature names:\", feature_names)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "# Create the SelectKBest object with k=2 (select top 2 features)\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "# Fit the selector to the training data and transform it\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "print(\"\\nSelected feature names:\", [feature_names[i] for i in selected_feature_indices])\n",
    "\n",
    "# Train a classifier using the selected features\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train_selected, y_train)\n",
    "# Transform the test data using the selected features\n",
    "X_test_selected = selector.transform(X_test)\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy of KNeighborsClassifier after selected feature names:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the Iris dataset, which contains samples of iris flowers with four features and a target variable representing the species of iris flowers. We demonstrate the process of feature selection using SelectKBest with the ANOVA F-value (f_classif) score function for classification tasks. We store the original feature names in the feature_names variable. After selecting the top two features using SelectKBest, we train a KNeighborsClassifier model that predicts with a good accuracy.\n",
    "\n",
    "### 1.2 SelectPercentile\n",
    "\n",
    "SelectPercentile is similar to SelectKBest, but it selects the top features based on a percentile of the highest-scoring features. This method is useful when you don't know the exact number of features you want to keep, but you want to retain a certain percentage of the best features.\n",
    "\n",
    "Parameters:\n",
    "- score_func: The scoring function used to evaluate the features' importance. Common choices for classification tasks include f_classif (ANOVA F-value) and mutual_info_classif (mutual information).\n",
    "- percentile: The percentage of top features to select.\n",
    "\n",
    "Advantages:\n",
    "- Allows selecting a specified percentage of top features, which is useful when you want to retain a proportion of the best features.\n",
    "- Handles scenarios where the optimal number of features is unknown or varies across datasets.\n",
    "\n",
    "Disadvantages:\n",
    "- Similar to SelectKBest, it assumes that the selected features are independent, which might not always be the case.\n",
    "- May lead to the exclusion of potentially relevant features if the percentile threshold is too stringent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "print(\"Original feature names:\", feature_names)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Create the SelectPercentile object with percentile=50 (select top 50% of features)\n",
    "selector = SelectPercentile(score_func=f_classif, percentile=50)\n",
    "# Fit the selector to the training data and transform it\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "print(\"\\nSelected feature names:\", selected_feature_names)\n",
    "\n",
    "# Train a KNN classifier using the selected features\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train_selected, y_train)\n",
    "# Transform the test data using the selected features\n",
    "X_test_selected = selector.transform(X_test)\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy of KNeighborsClassifier after feature selection:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the Iris dataset, which contains samples of iris flowers with four features and a target variable representing the species of iris flowers. We demonstrate the process of feature selection using SelectPercentile with the ANOVA F-value (f_classif) score function for classification tasks. We store the original feature names in the feature_names variable. After selecting the top two features using SelectKBest, we train a KNeighborsClassifier model that predicts with a good accuracy.\n",
    "\n",
    "### 1.3 Summary\n",
    "\n",
    "Both SelectKBest and SelectPercentile are useful feature selection methods to enhance model performance, reduce overfitting, and improve the interpretability of the model. Choose the appropriate method based on the number of features you want to retain or the percentage of top features you want to keep in your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
