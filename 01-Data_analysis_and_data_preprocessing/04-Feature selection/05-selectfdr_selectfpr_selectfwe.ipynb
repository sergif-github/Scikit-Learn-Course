{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 4: Feature selection\n",
    "\n",
    "### Part 5: SelectFdr, SelectFpr, and SelectFwe\n",
    "\n",
    "In this part, we will explore three statistical methods for feature selection: SelectFdr, SelectFpr, and SelectFwe. These methods are based on statistical hypothesis testing and control the family-wise error rate (FWER) or false discovery rate (FDR).\n",
    "\n",
    "### 5.1 SelectFdr, SelectFpr, and SelectFwe\n",
    "\n",
    "1. SelectFdr (False Discovery Rate): This method controls the expected proportion of false discoveries among the rejected hypotheses (false positives). It is used when you want to control the FDR, which is the ratio of false positives to the total number of rejected hypotheses. The higher the FDR, the more false positives you are willing to tolerate among the selected features.\n",
    "    \n",
    "2. SelectFpr (False Positive Rate): This method controls the expected proportion of false positives among all tested features. It is used when you want to control the FPR, which is the ratio of false positives to the total number of features selected. The higher the FPR, the more false positives you are willing to tolerate among the selected features.\n",
    "   \n",
    "3. SelectFwe (Family-wise Error Rate): This method controls the probability of at least one false positive among all tested features. It is used when you want to control the FWER, which is the probability of making at least one false positive (Type I error) among all the hypotheses tested. The lower the FWER, the more stringent the selection criteria for the features.\n",
    "\n",
    "Parameters:\n",
    "- estimator: The base estimator used for feature selection (e.g., a classifier or regressor).\n",
    "- alpha: \n",
    "    - In SelectFdr: The significance level or threshold for controlling the FDR. Features with p-values lower than alpha will be selected.\n",
    "    - In SelectFpr: The significance level or threshold for controlling the FPR. Features with p-values lower than alpha will be selected.\n",
    "    - In SelectFwd: The significance level or threshold for controlling the FWER. Features with p-values lower than alpha will be selected.\n",
    "\n",
    "Let's illustrate how to use these feature selection methods with a simple example using the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFdr, SelectFpr, SelectFwe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"All features:\")\n",
    "print(data.feature_names)\n",
    "\n",
    "# Create the logistic regression model\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# SelectFdr (False Discovery Rate) feature selection\n",
    "selector_fdr = SelectFdr(alpha=0.05)\n",
    "X_train_selected_fdr = selector_fdr.fit_transform(X_train, y_train)\n",
    "X_test_selected_fdr = selector_fdr.transform(X_test)\n",
    "# Train the classifier on the selected features using Fdr\n",
    "clf.fit(X_train_selected_fdr, y_train)\n",
    "y_pred_fdr = clf.predict(X_test_selected_fdr)\n",
    "accuracy_fdr = accuracy_score(y_test, y_pred_fdr)\n",
    "print(\"\\nSelected features using SelectFdr:\")\n",
    "print(selector_fdr.get_support(indices=True))\n",
    "\n",
    "# SelectFpr (False Positive Rate) feature selection\n",
    "selector_fpr = SelectFpr(alpha=0.05)\n",
    "X_train_selected_fpr = selector_fpr.fit_transform(X_train, y_train)\n",
    "X_test_selected_fpr = selector_fpr.transform(X_test)\n",
    "# Train the classifier on the selected features using Fpr\n",
    "clf.fit(X_train_selected_fpr, y_train)\n",
    "y_pred_fpr = clf.predict(X_test_selected_fpr)\n",
    "accuracy_fpr = accuracy_score(y_test, y_pred_fpr)\n",
    "print(\"\\nSelected features using SelectFpr:\")\n",
    "print(selector_fpr.get_support(indices=True))\n",
    "\n",
    "# SelectFwe (Family-wise Error Rate) feature selection\n",
    "selector_fwe = SelectFwe(alpha=0.05)\n",
    "X_train_selected_fwe = selector_fwe.fit_transform(X_train, y_train)\n",
    "X_test_selected_fwe = selector_fwe.transform(X_test)\n",
    "# Train the classifier on the selected features using Fwe\n",
    "clf.fit(X_train_selected_fwe, y_train)\n",
    "y_pred_fwe = clf.predict(X_test_selected_fwe)\n",
    "accuracy_fwe = accuracy_score(y_test, y_pred_fwe)\n",
    "print(\"\\nSelected features using SelectFwe:\")\n",
    "print(selector_fwe.get_support(indices=True))\n",
    "\n",
    "print(\"\\nAccuracy using SelectFdr:\", accuracy_fdr)\n",
    "print(\"Accuracy using SelectFpr:\", accuracy_fpr)\n",
    "print(\"Accuracy using SelectFwe:\", accuracy_fwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use Logistic Regression as the base estimator for feature selection. We demonstrate how to use SelectFdr, SelectFpr, and SelectFwe from the sklearn.feature_selection module to select features based on different significance levels (alpha). We then train the logistic regression classifier on the selected features and evaluate the accuracy of the model on the test set.\n",
    "\n",
    "The SelectFdr, SelectFpr, and SelectFwe methods take the significance level alpha as a parameter to control the false discovery rate, false positive rate, and family-wise error rate, respectively. By using these methods, we can choose features based on their statistical significance and control the risk of overfitting by selecting the most relevant features for the model. The accuracy results for each feature selection method can be compared to determine which method performs best for the given dataset and model.\n",
    "\n",
    "### 5.3 Summary\n",
    "\n",
    "The SelectFdr, SelectFpr, and SelectFwe feature selection methods provide different ways to control the false discovery rate, false positive rate, and family-wise error rate, respectively. By using these methods, you can tailor your feature selection strategy based on the desired level of statistical significance and the type of errors you want to control in your machine learning model. The desired level of statistical significance requires careful tuning to avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
