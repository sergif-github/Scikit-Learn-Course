{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 4: Feature selection\n",
    "\n",
    "### Part 6: Mutual Information\n",
    "\n",
    "Mutual Information is a powerful feature selection technique that measures the dependency between a feature and the target variable. Mutual Information captures both linear and non-linear relationships, making it well-suited for detecting complex dependencies in the data.\n",
    "\n",
    "Mutual Information is a measure of the amount of information that can be gained about one random variable (e.g., the target variable) by observing another random variable (e.g., a feature). It quantifies the reduction in uncertainty about the target variable when we know the feature's value. A higher Mutual Information value indicates a stronger dependency between the feature and the target, suggesting that the feature is informative for predicting the target.\n",
    "\n",
    "Considerations: \n",
    "- Computing Mutual Information can be computationally expensive for large datasets with high-dimensional features.\n",
    "- The performance of Mutual Information can be affected by the scale of the features, requiring proper feature scaling or normalization.\n",
    "\n",
    "### 6.1 Using mutual information\n",
    "\n",
    "Scikit-Learn provides the mutual_info_classif and mutual_info_regression functions to compute Mutual Information between features and a categorical or continuous target variable, respectively.\n",
    "\n",
    "Let's demonstrate how to use Mutual Information for feature selection on a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Compute Mutual Information scores\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display the scores for each feature\n",
    "mi_df = pd.DataFrame({'Feature': data.feature_names, 'Mutual_Information': mi_scores})\n",
    "mi_df.sort_values(by='Mutual_Information', ascending=False, inplace=True)\n",
    "\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mutual Information scores indicate the importance of each feature with respect to the target variable (iris flower type). Higher scores suggest that the feature contains valuable information for predicting the target.\n",
    "\n",
    "Mutual Information provides a ranking of features based on their relevance to the target variable, facilitating feature selection.\n",
    "\n",
    "### 6.2 Summary\n",
    "\n",
    "Mutual Information is a valuable feature selection technique that captures both linear and non-linear dependencies between features and the target variable. It is particularly useful for complex datasets with interacting features. By leveraging the power of Mutual Information in Scikit-Learn, you can effectively identify informative features for your machine learning tasks and improve model performance. However, it's essential to consider the computational cost and normalization requirements when applying Mutual Information to large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
