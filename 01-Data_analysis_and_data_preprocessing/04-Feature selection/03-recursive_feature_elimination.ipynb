{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 4: Feature selection\n",
    "\n",
    "### Part 3: Recursive Feature Elimination (RFE)\n",
    "\n",
    "In this part, we will explore the concept of Recursive Feature Elimination (RFE), a powerful technique used to recursively select the most important features by repeatedly fitting the model and removing the least significant features. Recursive Feature Elimination is particularly useful when dealing with complex datasets with a large number of features.\n",
    "\n",
    "### 2.1 Using Recursive Feature Elimination (RFE)\n",
    "\n",
    "Recursive Feature Elimination is a feature selection technique that works by recursively fitting the model with the remaining features and ranking them based on their importance. At each iteration, the least significant features are removed until the desired number of features is reached.\n",
    "\n",
    "The key idea behind Recursive Feature Elimination is to identify the most relevant features that contribute the most to the model's performance. By iteratively eliminating features, this technique can help to improve model efficiency, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "Advantages:\n",
    "- RFE is a powerful feature selection technique that recursively eliminates less important features, leading to a more parsimonious and interpretable model.\n",
    "- It works well with models that provide feature importances, making it flexible to use with various algorithms.\n",
    "\n",
    "Disadvantages:\n",
    "- The computational complexity of RFE can be high for large datasets or complex models, as it involves fitting the model multiple times.\n",
    "- RFE's performance may depend on the choice of the ranking model, and it may not be suitable for models without feature importance measures.\n",
    "\n",
    "The RFE process involves the following steps:\n",
    "\n",
    "1. Fit a model (e.g., linear regression or SVM) to the entire feature set.\n",
    "2. Rank the features based on their importance scores obtained from the model.\n",
    "3. Remove the least important feature(s) from the feature set.\n",
    "4. Repeat steps 1-3 until the desired number of features is reached.\n",
    "\n",
    "\n",
    "The primary parameters of the RFE class in scikit-learn are as follows:\n",
    "\n",
    "- estimator: This is the base estimator or model used to determine the feature importance. It should be a supervised learning estimator with a coef_ or feature_importances_ attribute. Common choices are linear models, decision trees, and random forests.\n",
    "- n_features_to_select: The number of top features to select. By default, it is set to None, which means half of the features will be selected.\n",
    "- step: The number of features to remove at each iteration. The default value is 1, which means one feature is removed at each iteration. You can set it to an integer to remove a fixed number of features at each step.\n",
    "- verbose: Controls the verbosity of the output during the feature selection process. If True, it will print updates during the selection process. If False, no output will be displayed.\n",
    "\n",
    "Here's how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create the model for feature ranking (e.g., Logistic Regression)\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "# Create the RFE object with the model and number of features to select\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "# Fit the RFE object to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "# Get the selected features\n",
    "selected_features = rfe.support_\n",
    "\n",
    "# Print the selected feature indices and their names\n",
    "print(\"All features:\")\n",
    "print(data.feature_names)\n",
    "print(\"\\nSelected features\")\n",
    "for x in np.where(selected_features)[0]:\n",
    "    print(\"\\t\",data.feature_names[x])\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = rfe.transform(X_train)\n",
    "X_test_selected = rfe.transform(X_test)\n",
    "\n",
    "# Train a classifier using the selected features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_selected, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nLogisticRegression model accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the Iris dataset and applied Recursive Feature Elimination (RFE) with a Logistic Regression model to select the top 2 features. We then trained a new Logistic Regression model using only the selected features and evaluated its accuracy on the test set. RFE helps us identify the most relevant features for classification tasks, improving model performance and interpretability.\n",
    "\n",
    "### 2.2 RFE with Cross-Validation\n",
    "\n",
    "To avoid overfitting and obtain a more reliable feature ranking, Recursive Feature Elimination is often combined with cross-validation. In each iteration, the model is trained and evaluated using cross-validation, which provides a more robust estimate of feature importance.\n",
    "\n",
    "Scikit-Learn provides the RFECV class for performing Recursive Feature Elimination with Cross-Validation. \n",
    "\n",
    "RFECV Parameters:\n",
    "- estimator: The machine learning estimator used to evaluate feature importance and perform feature selection. This estimator must have a coef_ or feature_importances_ attribute after fitting. For example, LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, etc.\n",
    "- step: The number of features to remove at each iteration. The default value is 1, which means one feature is removed at each iteration.\n",
    "- cv: The cross-validation strategy used to evaluate the performance of different feature subsets. It can be an integer (to specify the number of folds for k-fold cross-validation) or a cross-validation object from sklearn.model_selection (e.g., KFold, StratifiedKFold, etc.).\n",
    "- scoring: The scoring metric used to evaluate the performance during cross-validation. The default value is None, which uses the estimator's default scoring method (accuracy for classifiers and R-squared for regressors). You can specify any available scoring metric from sklearn.metrics.\n",
    "- min_features_to_select: The minimum number of features to select. The RFECV process will stop when the number of selected features is less than or equal to this value.\n",
    "- verbose: Controls the verbosity of the output during the fitting process. Set to 0 for no output, and higher values for more detailed output.\n",
    "\n",
    "Advantages of RFECV:\n",
    "- Automatic Feature Selection: RFECV automatically performs feature selection by recursively removing less important features, saving you from manual feature selection.\n",
    "- Cross-Validation: It performs cross-validation to evaluate feature subsets, making it more robust and less prone to overfitting compared to simple feature ranking methods.\n",
    "- Optimal Feature Subset: RFECV selects the optimal number of features that maximize the performance metric based on cross-validation.\n",
    "\n",
    "Disadvantages of RFECV:\n",
    "- Computationally Expensive: RFECV can be computationally expensive, especially for large datasets or when using complex models with high-dimensional feature spaces.\n",
    "- Estimator Choice: The choice of estimator can influence the results, so it's essential to choose an appropriate estimator for the specific problem.\n",
    "\n",
    "Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create the logistic regression model (or any other estimator)\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "# Create the RFECV object with the logistic regression model and perform 5-fold cross-validation\n",
    "rfecv = RFECV(estimator=model, cv=5)\n",
    "# Fit the RFECV object to the training data\n",
    "rfecv.fit(X_train, y_train)\n",
    "# Get the selected features\n",
    "selected_features = rfecv.support_\n",
    "\n",
    "# Print the selected feature indices and their names\n",
    "print(\"All features:\")\n",
    "print(data.feature_names)\n",
    "# Print the optimal number of features selected by RFECV\n",
    "print(\"\\nOptimal number of features:\", rfecv.n_features_)\n",
    "print(\"\\nSelected features:\")\n",
    "for x in np.where(selected_features)[0]:\n",
    "    print(\"\\t\", data.feature_names[x])\n",
    "\n",
    "# Transform the training and test sets to include only the selected features\n",
    "X_train_selected = rfecv.transform(X_train)\n",
    "X_test_selected = rfecv.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier on the reduced set of features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_selected, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nLogisticRegression model accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use RFECV instead of RFE, and we specify the number of folds for cross-validation using the cv parameter (in this case, 5-fold cross-validation). RFECV automatically selects the optimal number of features based on cross-validation performance, and you can access the number of selected features using rfecv.n_features_.\n",
    "\n",
    "### 2.3 Summary\n",
    "\n",
    "Recursive Feature Elimination is a powerful technique for selecting the most important features by iteratively fitting the model and eliminating the least significant features. It can help improve model efficiency, reduce overfitting, and enhance interpretability. Is a powerful tool especially when combined with cross-validation (RFECV).  Using RFECV ensures that the feature selection process is more robust and takes into account the performance on multiple folds of the data, leading to potentially better generalization and performance on unseen data. However, it may require more computation time, and the choice of estimator can impact the final results, so it's essential to experiment with different estimators and parameter settings to find the best combination for your specific task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
