{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 2: Feature scaling and normalization\n",
    "\n",
    "### Part 2: Min-Max Scaling\n",
    "\n",
    "In this part, we will explore the concept of Min-Max scaling, a data preprocessing technique used to transform features to a specified range.\n",
    "\n",
    "### 2.1 Understanding Min-Max Scaling\n",
    "\n",
    "Min-Max scaling is a technique used to transform numerical features to a specified range. It involves linearly scaling the features to fit within a specific interval, typically between 0 and 1. It maps the minimum value of the feature to 0 and the maximum value to 1, and linearly scales all other values in between. Also Min-Max scaling preserves the shape of the distribution.\n",
    "\n",
    "The key idea behind Min-Max scaling is to bring all features to a common range, making them comparable and avoiding the dominance of features with larger magnitudes. It is particularly useful when the absolute values or ranges of features are important for the learning algorithm.\n",
    "\n",
    "### 2.2 StandardScaler vs Min-Max Scaling\n",
    "\n",
    "Both Min-Max scaling and StandardScaler are common techniques used for feature scaling in machine learning. They have different effects on the data and serve different purposes. Let's compare Min-Max scaling and StandardScaler:\n",
    "\n",
    "StandardScaler:\n",
    "- Advantages:\n",
    "    - It standardizes the data to have zero mean and unit variance, making it suitable for algorithms that assume a Gaussian distribution or require features to be on the same scale.\n",
    "    - It is less sensitive to outliers compared to Min-Max scaling because it uses the mean and standard deviation, which are robust to extreme values.\n",
    "- Disadvantages:\n",
    "    - It may not preserve the original distribution of the data, especially if the data is not normally distributed.\n",
    "\n",
    "Min-Max Scaling:\n",
    "- Advantages:\n",
    "    - It preserves the original distribution of the data.\n",
    "    - It can be useful for algorithms that require features to be on the same scale, like neural networks and distance-based algorithms.\n",
    "- Disadvantages:\n",
    "    - It is sensitive to outliers, as the range is determined by the minimum and maximum values. Outliers can disproportionately impact the scaling.\n",
    "\n",
    "Which one to use depends on the specific characteristics of your data and the requirements of the machine learning algorithm you are using. If the algorithm is sensitive to feature scales and assumes Normal / Gaussian distribution , then StandardScaler may be more appropriate. However, if you want to preserve the original data distribution and have a specific range in mind, then Min-Max scaling could be a better choice.\n",
    "\n",
    "Several machine learning algorithms assume or work better with normally distributed data or maintain the original dataset distribution. Here are some examples:\n",
    "\n",
    "- Linear Regression: Linear regression assumes that the relationship between the independent variables and the dependent variable is linear and normally distributed with constant variance. Using normally distributed features can help in accurate parameter estimation.\n",
    "\n",
    "- Logistic Regression: Similar to linear regression, logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. While it does not require normally distributed features, maintaining the original data distribution can be helpful for interpretability.\n",
    "\n",
    "- Gaussian Naive Bayes: Naive Bayes classifiers, particularly the Gaussian variant, assume that the features follow a Gaussian distribution. Maintaining the original data distribution can be essential for accurate probability estimation.\n",
    "\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a popular dimensionality reduction technique that preserves the local structure of data points. While it does not assume normal distribution, it tends to perform better when the data distribution is maintained.\n",
    "\n",
    "- Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that seeks orthogonal components to represent the data variance. PCA does not assume normal distribution, but it can be sensitive to the scaling of the features, so it is important to maintain the original data distribution during preprocessing.\n",
    "\n",
    "It's important to note that while some algorithms work better with normal distributions many modern machine learning algorithms, such as decision trees, random forests, and gradient boosting, are robust to the data distribution.\n",
    "\n",
    "The choice of algorithm and preprocessing techniques should be guided by your specific data and problem domain. It is always a good practice to experiment with different preprocessing strategies and observe their impact on model performance.\n",
    "\n",
    "### 2.3 Using min-max scaler\n",
    "\n",
    "To apply Min-Max scaling, we need a dataset with numerical features. \n",
    "\n",
    "Scikit-Learn provies the MinMaxScaler class for performing Min-Max scaling. Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset with two features (columns)\n",
    "data = np.array([[10, 2],\n",
    "                 [20, 5],\n",
    "                 [30, 10],\n",
    "                 [40, 15],\n",
    "                 [50, 20]])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "# Create a MinMaxScaler object to scale the data to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler to the data and compute the minimum and maximum values for scaling\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using the learned parameters to scale it to the range [0, 1]\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(\"\\nScaled Data (Min-Max Scaling):\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we created a sample dataset with two features and five samples. We then used the MinMaxScaler to scale the data to the range [0, 1]. First, we created a MinMaxScaler object, and then we called the fit method to compute the minimum and maximum values of each feature in the dataset. After fitting, we used the transform method to apply the Min-Max scaling to the data, resulting in a new dataset with all values scaled to the range [0, 1].\n",
    "\n",
    "### 2.4 Summary\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to transform numerical features to a specified range. It brings features within the desired interval, making them directly comparable and avoiding the dominance of features with larger magnitudes. Scikit-Learn provides the MinMaxScaler class for performing Min-Max scaling easily. Understanding the concepts, training, and parameter tuning is crucial for effectively using Min-Max scaling in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
