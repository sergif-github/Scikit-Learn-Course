{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 3: Encoding categorical variables\n",
    "\n",
    "### Part 5: Hashing Encoding\n",
    "\n",
    "Hashing Encoding is a data preprocessing technique used to convert categorical variables into a numerical representation using hashing functions. Hashing Encoding is particularly useful when dealing with high-dimensional categorical variables or when memory efficiency is a concern.\n",
    "\n",
    "### 5.1 Understanding Hashing Encoding\n",
    "\n",
    "Hashing Encoding is a technique used to convert categorical variables into a numerical representation using hashing functions. It maps each category to a fixed number of bins or \"hashes\" based on the output of a hash function. The resulting numerical representation can be used as input to machine learning algorithms.\n",
    "\n",
    "The key idea behind Hashing Encoding is to transform categorical variables into a fixed-dimensional space, regardless of the number of unique categories. This technique is especially useful when dealing with high-cardinality variables or situations where memory efficiency is crucial.\n",
    "\n",
    "### 5.2 Using hasing encoding\n",
    "\n",
    "To apply Hashing Encoding, we need a dataset with categorical variables. The encoding process involves applying a hash function to each category and mapping the resulting hash value to a fixed number of bins. The number of bins determines the dimensionality of the encoded representation.\n",
    "\n",
    "The scikit learn FeatureHasher maps each category to a specific index in a fixed-size feature vector, and the hashing process helps reduce memory usage and computational complexity. \n",
    "\n",
    "Here's an example of how to use scikit-learn's FeatureHasher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Sample data with a categorical feature\n",
    "data = [{'color': 'Red'}, {'color': 'Green'}, {'color': 'Blue'}, {'color': 'Green'}, {'color': 'Red'}, {'color': 'Blue'}]\n",
    "\n",
    "# Create the FeatureHasher object\n",
    "hasher = FeatureHasher(n_features=3, input_type='dict')\n",
    "# Transform the data using FeatureHasher\n",
    "hashed_features = hasher.transform(data)\n",
    "# Convert the hashed features to a dense NumPy array for better visualization\n",
    "hashed_features_array = hashed_features.toarray()\n",
    "\n",
    "# Print the original data and the hashed features\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nHashed Features:\")\n",
    "print(hashed_features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a sample dataset with a categorical feature 'color'. We use the FeatureHasher with n_features=3 to specify the size of the resulting feature vector. The input_type='dict' indicates that our data is in the form of a list of dictionaries, where each dictionary contains the categorical feature.\n",
    "\n",
    "The output will be a dense NumPy array where each row corresponds to the hashed representation of the corresponding categorical feature in the original data. The number of columns in the array is equal to the specified n_features, and each cell contains the numerical value after hashing the corresponding category.\n",
    "\n",
    "Note that the hashing process may lead to collisions, where different categories are mapped to the same index, but this is usually managed by using a sufficiently large value for n_features to reduce the likelihood of collisions and maintain good representation properties. Also, it is important to keep in mind that FeatureHasher does not provide a way to map the hashed features back to the original categories, as it is a one-way transformation. Therefore, it is generally used in scenarios where interpretability of the transformed features is not a critical concern.\n",
    "\n",
    "Regenerate response\n",
    "### 5.3 Choosing Parameters\n",
    "\n",
    "The most important parameter in Hashing Encoding is the number of features or bins (n_features), which determines the dimensionality of the encoded representation. Choosing an appropriate number of features depends on the cardinality of the categorical variable and the desired trade-off between memory efficiency and collision risk.\n",
    "\n",
    "### 5.4 Handling High-Cardinality Variables\n",
    "\n",
    "Hashing Encoding is particularly useful when dealing with high-dimensional categorical variables or variables with a large number of unique categories. By mapping categories to a fixed number of bins, Hashing Encoding ensures a consistent dimensionality regardless of the number of categories. However, it is important to note that collisions can occur, where different categories are hashed to the same bin.\n",
    "\n",
    "### 5.5 Smmary\n",
    "\n",
    "Hashing Encoding is a data preprocessing technique used to convert categorical variables into a numerical representation using hash functions. It maps categories to a fixed number of bins, allowing for a consistent dimensionality regardless of the number of unique categories. Libraries like category_encoders and scikit-learn provide convenient functions and classes for performing Hashing Encoding in Python. Understanding the concepts, choosing the number of features, and handling high-cardinality variables are crucial for effectively using Hashing Encoding in practice.\n",
    "\n",
    "In the next part, we will explore other data preprocessing techniques provided by Scikit-Learn.\n",
    "\n",
    "Feel free to practice implementing Hashing Encoding using libraries like category_encoders or scikit-learn. Experiment with different datasets and observe the effects of the encoding on the categorical variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
