{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Data Analysis and Data Preprocessing\n",
    "\n",
    "## Section 1: Handling missing data\n",
    "\n",
    "### Part 6: Regesssors and classifiers\n",
    "\n",
    "In this part, we will explore two approaches to handle missing data using machine learning algorithms: Handling Missing Data with Regressors and Handling Missing Data with Classifiers.\n",
    "\n",
    "We will explore in the following sections different machine learning models for solving regression or classification problems. For now, keep in mind that they can predict data, so they can also predict missing data in a data set.\n",
    "\n",
    "Remember that the models that we are going to create are for handling missing data in our original dataset. To compute new predictions then we need create another model with all the data completed (now without missing values).\n",
    "\n",
    "### 6.1 Handling Missing Data with Regressors\n",
    "\n",
    "When dealing with continuous or numerical features that have missing values, one effective approach is to use regressors to predict the missing values. The process involves the following steps:\n",
    "\n",
    "1. Identify the numerical features with missing values in the dataset.\n",
    "2. Split the dataset into two parts: one with complete data for the feature being imputed (training set) and one with missing values (test set).\n",
    "3. For each numerical feature with missing values, use the complete data from the training set to build a regression model.\n",
    "4. Use the regression model to predict the missing values in the test set.\n",
    "\n",
    "The advantage of using regressors for imputation lies in the ability to capture the relationships between features and utilize this information to make more accurate predictions. Popular regressors for this task include Linear Regression, Decision Trees, Random Forests, and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2 * x + 5 + np.random.normal(0, 2, 100)\n",
    "# Introduce missing values to 'y' feature only\n",
    "missing_indices = np.random.choice(100, size=20, replace=False)\n",
    "y_with_missing = y.copy()\n",
    "y_with_missing[missing_indices] = np.nan\n",
    "# Convert the NumPy arrays to a DataFrame\n",
    "df = pd.DataFrame({'x': x, 'y': y_with_missing})\n",
    "\n",
    "# Split the dataset into complete data (training set) and missing data (test set)\n",
    "train_data = df.dropna()\n",
    "test_data = df[df['y'].isnull()]\n",
    "# Create a linear regression model\n",
    "regressor = LinearRegression()\n",
    "# Fit the model on the training data\n",
    "regressor.fit(train_data[['x']].values, train_data['y'].values)\n",
    "# Predict the missing values using the fitted model\n",
    "imputed_values = regressor.predict(test_data[['x']].values)\n",
    "# Fill in the missing values in the original DataFrame\n",
    "#df.loc[df['y'].isnull(), 'y'] = imputed_values\n",
    "\n",
    "# Plot the original data with missing values and the imputed data\n",
    "plt.scatter(df['x'], df['y'], label='Original Data with Missing Values', color='blue')\n",
    "plt.scatter(test_data['x'], imputed_values, label='Imputed Values', color='red', marker='x', s=100)\n",
    "plt.plot(x, regressor.predict(x.reshape(-1, 1)), label='Linear Regression Line', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Imputation of Missing Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first generate synthetic data with a linear relationship between 'x' and 'y'. We then introduce 20 missing values in the 'y' feature using randomly chosen indices. We use Linear Regression to predict and impute the missing values in the 'y' feature based on the available data. The red 'x' markers represent the imputed values, and the blue points represent the original data with missing values. The linear regression imputation helps fill in the missing values with estimates based on the relationship between 'x' and 'y'.\n",
    "The green line represents the linear regression fitted to the original data points. It helps us visualize how the imputed values fit into the overall trend of the data.\n",
    "\n",
    "### 6.2 Handling Missing Data with Classifiers\n",
    "\n",
    "For categorical features with missing values, the approach is similar, but we use classification algorithms instead of regressors. The steps are as follows:\n",
    "\n",
    "1. Identify the categorical features with missing values in the dataset.\n",
    "2. Split the dataset into two parts: one with complete data for the feature being imputed (training set) and one with missing values (test set).\n",
    "3. For each categorical feature with missing values, use the complete data from the training set to build a classification model.\n",
    "4. Use the classification model to predict the missing categories in the test set.\n",
    "\n",
    "Using classifiers for imputation provides the advantage of preserving relationships between features and maintaining the categorical nature of the data. Common classifiers used for this purpose are Decision Trees, Random Forests, Naive Bayes, and Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(1)\n",
    "X1 = np.random.normal(loc=[0, 0], scale=1, size=(50, 2))\n",
    "X2 = np.random.normal(loc=[2, 2], scale=1, size=(50, 2))\n",
    "X3 = np.random.normal(loc=[5, 5], scale=1, size=(50, 2))\n",
    "X = np.vstack([X1, X2, X3])\n",
    "y = np.repeat([1, 2, 3], 50)\n",
    "# Introduce missing class labels randomly\n",
    "missing_indices = np.random.choice(150, size=10, replace=False)\n",
    "y_with_missing = y.copy()\n",
    "y_with_missing[missing_indices] = -1  # Use -1 to indicate missing class labels\n",
    "# Convert the NumPy arrays to a DataFrame\n",
    "df = pd.DataFrame(X, columns=['X1', 'X2'])\n",
    "df['Class'] = y_with_missing\n",
    "\n",
    "# Split the dataset into complete data (training set) and missing data (test set)\n",
    "train_data = df[df['Class'] != -1]\n",
    "test_data = df[df['Class'] == -1]\n",
    "# Split the training and test data into features (X) and class labels (y)\n",
    "X_train = train_data[['X1', 'X2']]\n",
    "y_train = train_data['Class']\n",
    "X_test = test_data[['X1', 'X2']]\n",
    "\n",
    "# Create an SVM classifier\n",
    "classifier = SVC()\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predict the missing class labels using the fitted classifier\n",
    "imputed_labels = classifier.predict(X_test)\n",
    "df_final = df.copy()\n",
    "# Fill in the missing class labels in the original DataFrame\n",
    "df_final.loc[df_final['Class'] == -1, 'Class'] = imputed_labels\n",
    "# Convert the class labels back to integers (optional)\n",
    "df_final['Class'] = df_final['Class'].astype(int)\n",
    "\n",
    "# Evaluate the classifier's accuracy on the complete data\n",
    "y_true = y[df_final['Class'] != -1].astype(int)\n",
    "y_pred = df_final['Class'][df_final['Class'] != -1].astype(int)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy of SVM Classifier on Complete Data:\", accuracy)\n",
    "\n",
    "# Plot the data points with different classes and the imputed points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_final[df_final['Class'] == 1]['X1'], df_final[df_final['Class'] == 1]['X2'], label='Class 1', marker='o', s=100, color='blue')\n",
    "plt.scatter(df_final[df_final['Class'] == 2]['X1'], df_final[df_final['Class'] == 2]['X2'], label='Class 2', marker='o', s=100, color='green')\n",
    "plt.scatter(df_final[df_final['Class'] == 3]['X1'], df_final[df_final['Class'] == 3]['X2'], label='Class 3', marker='o', s=100, color='orange')\n",
    "plt.scatter(df[df['Class'] == -1]['X1'], df[df['Class'] == -1]['X2'], label='Imputed Points', marker='x', s=100, color='red')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Data Points with Different Classes and Imputed Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have generated a synthetic dataset with three classes, introduced missing class labels to the data, and used an SVM classifier to predict the missing class labels. The SVM classifier is trained on the data with complete class labels, and then used to predict the class labels of the missing data points. Finally, we fill in the missing class labels in the original DataFrame with the imputed values and evaluate the accuracy of the classifier on the complete data.\n",
    "\n",
    "\n",
    "### 6.3 Benefits of Machine Learning-Based Imputation\n",
    "\n",
    "1. Preservation of Relationships: By leveraging the power of machine learning algorithms, both regressors and classifiers can take into account the underlying relationships between features, leading to more accurate imputations.\n",
    "2. Flexibility: The use of various machine learning algorithms allows for flexibility in handling different types of missing data and handling both continuous and categorical features.\n",
    "3. Handling Multiple Features: These methods can handle missing data in multiple features simultaneously, capturing correlations between different features in the dataset.\n",
    "\n",
    "### 6.4 Caveats and Considerations\n",
    "\n",
    "While machine learning-based imputation techniques can be powerful, it is essential to be cautious when applying them:\n",
    "\n",
    "- Overfitting: There is a risk of overfitting, especially with complex models, leading to biased predictions. Regularization and cross-validation can help mitigate this issue.\n",
    "- Model Selection: Careful selection of the appropriate regression or classification model is crucial to obtain accurate imputations.\n",
    "- Data Distribution: The quality of the imputations depends on the underlying data distribution and the relationships between features.\n",
    "\n",
    "### 6.5 Summary\n",
    "\n",
    "In conclusion, handling missing data with machine learning algorithms offers a powerful and flexible approach that can significantly improve the completeness and usefulness of datasets. However, proper model selection, regularization, and evaluation are essential to ensure the reliability of the imputed data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
